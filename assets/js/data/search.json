[ { "title": "[IR light] 전등 리모컨 제작기 - 3", "url": "/posts/IR_light_3/", "categories": "Making, IR light, pyqt5, program, widget, serial, port, pyinstaller", "tags": "IR light", "date": "2023-04-19 00:00:00 +0900", "snippet": " 관련 포스팅 [IR light] 전등 리모컨 제작기 - 1 [IR light] 전등 리모컨 제작기 - 2 3. PC 프로그램 제작import sysfrom PyQt5.QtWidgets import (QApplication, QWidget, QGridLayout, QLabel, QComboBox, QPushButton, QDesktopWidget)from PyQt5.QtCore import QTimerfrom PyQt5.QtGui import QIconimport serial.tools.list_ports as spimport serialclass MyApp(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): self.ports = sorted(sp.comports()) self.myserial = serial.Serial(port=&#39;COM1&#39;, baudrate=9600) self.cb_serial = QComboBox(self) self.cb_serial.currentIndexChanged.connect(self.onActivated) for port, desc, _ in self.ports: self.cb_serial.addItem(&quot;{}: {}&quot;.format(port, desc)) self.timer = QTimer(self) self.timer.start(1000) self.timer.timeout.connect(self.control_cb_data) btn_on = QPushButton(&#39;&amp;amp;On&#39;, self) btn_on.clicked.connect(lambda: self.btn_clicked(&#39;1&#39;)) btn_off = QPushButton(&#39;&amp;amp;Off&#39;, self) btn_off.clicked.connect(lambda: self.btn_clicked(&#39;0&#39;)) btn_sleep = QPushButton(&#39;&amp;amp;Sleep light&#39;, self) btn_sleep.clicked.connect(lambda: self.btn_clicked(&#39;d&#39;)) btn_bright = QPushButton(&#39;&amp;amp;20% / 50% / 100%&#39;, self) btn_bright.clicked.connect(lambda: self.btn_clicked(&#39;b&#39;)) self.cb_timer = QComboBox(self) self.cb_timer.addItems([&quot;1 min&quot;, &quot;10 min&quot;, &quot;20 min&quot;, &quot;30 min&quot;]) btn_timer = QPushButton(&#39;&amp;amp;Set timer&#39;, self) btn_timer.clicked.connect(self.btn_timer_clicked) grid = QGridLayout() self.setLayout(grid) grid.addWidget(QLabel(&#39;Serial port&#39;), 0, 0, 1, 4) grid.addWidget(self.cb_serial, 1, 0, 1, 4) grid.addWidget(QLabel(&#39;On/Off&#39;), 2, 0, 1, 2) grid.addWidget(btn_on, 3, 0) grid.addWidget(btn_off, 3, 1) grid.addWidget(QLabel(&#39;Control brightness&#39;), 4, 0, 1, 2) grid.addWidget(btn_sleep, 5, 0, 1, 2) grid.addWidget(btn_bright, 6, 0, 1, 2) grid.addWidget(QLabel(&#39;Timer&#39;), 2, 2, 1, 2) grid.addWidget(self.cb_timer, 3, 2, 1, 2) grid.addWidget(btn_timer, 6, 2, 1, 2) self.setWindowTitle(&#39;Light remote&#39;) self.setWindowIcon(QIcon(&#39;./remote.png&#39;)) self.setGeometry(300, 300, 300, 200) self.show() def onActivated(self): self.myserial.setPort(self.ports[self.cb_serial.currentIndex()][0]) print(self.ports[self.cb_serial.currentIndex()][0]) def control_cb_data(self): now_ports = sorted(sp.comports()) if self.ports != now_ports: self.ports = now_ports self.cb_serial.clear() for port, desc, _ in self.ports: self.cb_serial.addItem(&quot;{}: {}&quot;.format(port, desc)) def btn_clicked(self, chr): self.myserial.write(str.encode(chr)) def btn_timer_clicked(self): if self.cb_timer.currentIndex() == 0: self.myserial.write(str.encode(&#39;v&#39;)) elif self.cb_timer.currentIndex() == 1: self.myserial.write(str.encode(&#39;s&#39;)) elif self.cb_timer.currentIndex() == 2: self.myserial.write(str.encode(&#39;m&#39;)) elif self.cb_timer.currentIndex() == 3: self.myserial.write(str.encode(&#39;l&#39;)) def location_on_the_screen(self): screen = QDesktopWidget().screenGeometry() widget = self.geometry() x = screen.width() - widget.width() y = 0 self.move(x, y) if __name__ == &#39;__main__&#39;: app = QApplication(sys.argv) ex = MyApp() ex.location_on_the_screen() sys.exit(app.exec_())IR remote program windowpyqt5 모듈을 사용하여 구현하였습니다. GUI 프로그램을 개발하는건 이번 기회가 처음이라 공부하면서 제작하였습니다. 다음 wikidocs를 주로 참고하였습니다. PyQt5 Tutorial - 파이썬으로 만드는 나만의 GUI 프로그램이하 내용은 기능과 사용한 위젯을 중심으로 기술해 보았습니다.3.1 시리얼 포트 설정serial.tools.list_ports.comports()를 이용하여 현재 컴퓨터에서 열려 있는 시리얼 포트를 받아올 수 있습니다. 받아온 시리얼 포트 정보는 콤보 박스 위젯을 통해 사용자에게 전달되며 사용자는 어떤 포트와 통신할지 선택할 수 있습니다. QTImer 클래스를 사용하여 1초마다 포트 상태를 체크하는데, 만약 상태가 달라졌다면 콤보 박스의 아이템을 갱신합니다.3.2 전등 전원버튼 위젯을 사용하여 On, Off 전원을 구현하였습니다. On/Off 버튼 클릭 시 미리 선택된 시리얼 포트로 &#39;1&#39;/&#39;0&#39; 1byte 신호를 보냅니다. IR 송신 모듈은 시리얼 값을 읽어와 대응하는 IR command를 송신합니다.3.3 전등 밝기 조절마찬가지로 버튼 위젯을 사용하여 구현하였습니다. 가장 어두운 밝기인 취침등(Sleep light) 설정과 20%/50%/100% 밝기를 순차적으로 선택할 수 있는 설정이 있습니다.3.4 타이머 조절전등이 꺼지는 시각을 미리 설정해놓는 타이머 기능이 있습니다. 1분, 10분, 20분, 30분 이렇게 4가지 설정이 가능하고, 콤보 박스 상에서 선택한 후 Set timer 버튼을 클릭 시 시리얼 신호가 송신됩니다.3.5 그 외 그리드 레이아웃을 사용하여 각 위젯를 배치하였습니다. 프로그램 실행 시 자동으로 모니터의 우측 상단에서 열리도록 하였습니다. pyinstaller를 사용하여 실행파일로 만들었습니다. 콘솔창이 출력되지 않게 하고, 프로그램 아이콘을 설정하기 위해 다음과 같은 명령어를 콘솔창에 입력해줍니다.pyinstaller -w --icon=./remote.ico IR_light.py4. 결과IR remote program result잘 작동합니다!!추가로 저는 프로그램 단축키를 등록해주었습니다. 단축키 등록 방법은 바탕화면에 바로가기 만들기 아이콘 우클릭 후 ‘속성’ ‘바로 가기’ 탭에서 ‘바로 가기’ 키를 누른 다음 원하는 단축키를 입력순서로 진행하면 됩니다. 저는 ctrl + alt + R 키로 등록하였습니다.지금까지 리모컨도 필요 없는 완전한 무선 전등 제어 시스템을 구축해 보았습니다 :D" }, { "title": "[IR light] 전등 리모컨 제작기 - 2", "url": "/posts/IR_light_2/", "categories": "Making, IR light, IR, sender, circuit, schematic, led, sendNEC, NEC, reflector", "tags": "IR light", "date": "2023-04-19 00:00:00 +0900", "snippet": " 관련 포스팅 [IR light] 전등 리모컨 제작기 - 1 2. IR 송신 모듈 제작2.1 IR 송신 회로IR sender circuit diagramIR sender circuit schematicIR sender circuit아두이노 DC 핀의 출력 전류는 최대 40mA 정도로 알려져 있습니다. 그러나 이 전류는 IR led에서 전등까지 적외선 신호를 보내기에 부족합니다. 따라서 전류를 증폭하는 회로를 NPN 트랜지스터를 사용하여 제작해 주었습니다. 제가 사용한 NPN 트랜지스터는 BC547로, 회로도 그림 기준 왼쪽부터 collector, base, emittor입니다. 다른 NPN 트랜지스터를 사용할 시 이에 맞추어 적용하면 될 것 같습니다.회로도 연결 결과 가변저항에 약 2.5V의 전압 강하가 발생하는걸 확인하였습니다. IR led가 작동하는 선에서 최대 전류가 흐르도록 가변저항을 조절한 결과, 가변저항을 30옴으로 맞추었습니다. 즉, 이 회로에서는 IR led에 2.5V/30Ω = 83.3mA의 전류가 인가됩니다.How to check IR ledIR led는 켜졌는지 육안으로 확인할 수 없습니다. 그러나 카메라를 통하여 관찰하면 IR led 불빛을 관찰할 수 있습니다.2.2 아두이노 코드#include &amp;lt;IRremote.h&amp;gt;IRsend irsend;void setup() { irsend.begin(3); Serial.begin(9600);}void loop() { if (Serial.available()) { char buf = Serial.read(); switch(buf) { case &#39;1&#39;: irsend.sendNEC2(0xDEA8, 0xFF, 3); //on break; case &#39;0&#39;: irsend.sendNEC2(0xDEA8, 0x1, 3); //off break; case &#39;d&#39;: irsend.sendNEC2(0xDEA8, 0x6, 1); //sleep break; case &#39;b&#39;: irsend.sendNEC2(0xDEA8, 0xA, 1); //20/50/100 break; case &#39;v&#39;: irsend.sendNEC2(0xDEA8, 0x1A, 1); //1min break; case &#39;s&#39;: irsend.sendNEC2(0xDEA8, 0x16, 1); //10min break; case &#39;m&#39;: irsend.sendNEC2(0xDEA8, 0x14, 1); //20min break; case &#39;l&#39;: irsend.sendNEC2(0xDEA8, 0x8, 1); //30min break; } }}시리얼 통신을 통해 문자를 입력하면 그에 대응되는 command가 IR 신호로 송신되는 코드입니다.IR 수신 코드와 마찬가지로 IRremote.h 라이브러리를 사용하여 IR 송신 코드를 작성하였습니다. 앞서 포스팅에서 sendNEC() 함수를 사용하여 송신할 수 있음을 파악하였습니다. 대부분의 기능에서 잘 작동하지만, 전등 전원을 켜거나 끌 때 가끔씩 입력이 씹히는 경우가 있었습니다. SendNEC() 함수의 NumberOfRepeats parameter를 조절하여 여러 번 송신하면 될거라 생각했지만, repeat protocol에 해당하는 신호가 송신되기 때문에 해결되지 못하였습니다.해결 방법은 sendNEC2() 함수를 사용하는 것입니다. 이 함수는 NumberOfRepeats가 2 이상이어도 repeat protocol 신호를 보내는 것이 아니라, 원래 신호를 반복하여 송신합니다. 그렇기 때문에 첫 신호가 무시되었다 하더라도 다음 신호를 인식하여 전등이 작동되게 됩니다. sendNEC2() 함수가 포함되어있는 ir_NEC.hpp의 주석에 다음과 같이 적혀 있습니다. NEC2 Send frame and repeat the frame for each requested repeat2.3 만능기판 회로도 설계 및 납땜IR sender back side boardIR sender front side boardIR sender back side board다음과 같이 만능기판 회로도를 설계하고 납땜하였습니다. 아두이노 나노는 핀헤더를 납땜한 다음 그 위에 장착하였습니다. IR led는 몰렉스 커넥터를 사용하여 탈착이 가능하도록 연결하였습니다.2.4 하드웨어 제작IR sender reflector IR 송신 세기를 향상시키기 위해 반사판을 달아주었습니다. 사진과 같이 탁구공을 반 자른 다음 알루미늄 호일로 감싸 IR 신호가 잘 반사되게 만들었습니다.IR sender module IR led의 각도를 조절할 수 있도록 볼 조인트 장치를 추가하였습니다. 기존에 있는 USB 통신 케이블의 길이를 줄이고 만능기판에 부착하였습니다.이렇게 최종적으로 완성된 IR 송신 모듈은 상단에 위치한 컴퓨터 USB 포트에 꽂아 사용할 수 있습니다. IR led 각도를 전등을 향하도록 볼 조인트를 조절하여 전등이 IR 신호를 잘 수신할 수 있도록 할 수 있습니다." }, { "title": "[IR light] 전등 리모컨 제작기 - 1", "url": "/posts/IR_light_1/", "categories": "Making, IR light, IR, receiver, NEC, protocol, IRremote", "tags": "IR light", "date": "2023-04-16 00:00:00 +0900", "snippet": "거실등이 고장나서 led 전등으로 바꿨더니 형광등보다 훨씬 밝고 좋더라고요.그래서 멀쩡한 제 방 전등도 led 전등으로 교체했습니다.일반 전등으로 교체하는건 재미 없으니까 뭐가 있을까 찾아보다가리모컨으로 조종할 수 있는 전등을 발견했습니다. 라이톤 LED조명 리모컨 사각방등 50W편리하고 좋지만 치명적인 문제가 있는데, 바로 전용 리모컨이 있어야 한다는 것입니다.주위에 리모컨이 없으면 찾으러 다녀야 하고,그럼 움직이지 않고 전등을 조종한다는 무선 리모컨의 의미가 잃어버리게 되죠.그래서 이번 프로젝트는컴퓨터 앞에 앉은 채 바로 전등을 컨트롤할 수 있는 시스템을 개발해보기로 하였습니다.큰 흐름을 미리 살펴보자면, 리모컨이 보내는 신호를 파악한다. 컴퓨터와 시리얼 통신이 가능한 IR 송신 모듈을 제작한다. PC 상에서 시리얼 통신으로 IR 송신 모듈에 신호를 보낼 수 있는 프로그램을 제작한다.순서로 진행될 예정입니다.1. 리모컨이 보내는 IR 신호 파악1.1 IR 수신 회로IR receiver circuit diagramIR receiver circuitIR 수광 모듈은 왼쪽부터 Vout, GND, VCC 핀으로 구성되어있습니다. Vout 핀을 아두이노 A0 핀에 연결합니다.1.2 아두이노 코드#include &amp;lt;IRremote.h&amp;gt;const int sensor = A0;IRrecv irrecv(sensor);void setup() { Serial.begin(9600); irrecv.begin(sensor, ENABLE_LED_FEEDBACK);}void loop() { if(irrecv.decode()) { irrecv.printIRResultShort(&amp;amp;Serial); irrecv.printIRSendUsage(&amp;amp;Serial); if (irrecv.decodedIRData.protocol == UNKNOWN) { Serial.println(F(&quot;Received noise or an unknown (or not yet enabled) protocol&quot;)); irrecv.printIRResultRawFormatted(&amp;amp;Serial, true); } Serial.println(); irrecv.resume(); }}IRremote.h 라이브러리를 사용하여 IR 수신 코드를 작성하였습니다. irrecv.printIRResultShort()는 IR 수신 데이터의 protocol, address, command, raw data를 serial 모니터 상에 출력합니다. irrecv.printIRSendUsage()는 수신 데이터를 똑같이 송신할 수 있는 IRremote 함수를 알려줍니다. 만약 알 수 없는 protocol 데이터가 수신될 경우 예외 메시지를 출력합니다. 대부분 노이즈에 해당합니다.1.3 결과IR receive result제가 사용하는 전등 리모컨은 NEC protocol로 통신한다는 것을 알 수 있습니다. address는 0xDEA8로 항상 동일하고, command는 버튼마다 다르게 나타납니다. 독특한 점은 버튼을 꾹 누르고 있으면 같은 신호가 연속해서 송신되는 것이 아니라, repeat하고 있다는 신호가 송신된다는 점입니다. IrSender.sendNEC() 함수를 이용해 통신할 수 있다는 사실까지 함께 알려주네요.리모컨 버튼에 대응되는 command를 아래에 정리해 보았습니다. 버튼 Command On 0xFF Off 0x1 취침등(가장 어두운 등) 0x6 20%/50%/100% 밝기 0xA 1분 타이머 0x1A 10분 타이머 0x16 20분 타이머 0x14 30분 타이머 0x8 20%/50%/100% 밝기 버튼은 처음 1번 시 20%, 2번 누를 시 50%, 3번 누를 시 100%, 4번 누를 시 20%… 방식으로 밝기를 전환할 수 있는 버튼입니다.1.4 NEC protocolNEC bit expressionNEC protocol에 대해 좀 더 자세하게 조사해보았습니다. 0은 562.5us HIGH 뒤에 562.5us동안 LOW, 1은 562.5us HIGH 뒤에 1687.5us LOW으로 표현합니다. Data frame은 그림과 같이 5 단계로 구성됩니다.NEC protocol Start : 9ms동안 HIGH 이후 4.5m LOW Address : 8Bit Address Data Inverse address : 8Bit Inverse Address Data Command : 8bit Command Data Inverse command : 8bit Command Data원 데이터와 반전된 데이터를 함께 송신하여 수신부에서 데이터가 제대로 수신되었는지 손실 여부를 파악하도록 합니다. 이럴 경우 0, 1 bit가 8개씩 일정하게 사용되므로 한 신호를 송신할 때 총 걸리는 시간이 67.5ms로 일정하게 유지됩니다.NEC repeat protocol데이터를 연속해서 송신하는 경우, Start 신호 이후 2250us동안 HIGH로 출력하는걸로 규정되어 있습니다." }, { "title": "[3D scanner] 3D scanner 제작기 - 4", "url": "/posts/3D_scanner_4/", "categories": "Making, 3D scanner", "tags": "3D scanner, hardware, Solidworks, result, 3D scanning, resolution, pointcloud, mesh", "date": "2023-04-12 00:00:00 +0900", "snippet": " 관련 포스팅 [3D scanner] 3D scanner 제작기 - 1 [3D scanner] 3D scanner 제작기 - 2 [3D scanner] 3D scanner 제작기 - 3 5. 결과5.1 전체 시스템 소개3D scanner image3D scanner operating diagram이번 프로젝트에 제작한 3D 스캐너는 그림과 같이 원통 좌표계를 따라 IR 거리 센서가 점을 스캔하고 이를 직교 좌표계로 변환한 pointcloud로 저장합니다. 이를 위해, 물체를 시계 방향으로 회전시키는 스텝 모터와 IR 거리 센서를 z 방향으로 움직이는 리드 스크류를 회전시키는 스텝 모터가 사용되었습니다. Plate의 z=0 지점부터 미리 설정한 θ-해상도에 맞춰 일정 각도마다 점을 스캔합니다. IR 거리 센서가 센서 carriage로부터 물체의 표면까지 거리를 측정하면 이미 알려진 plate 회전 중심과 carriage 사이 거리 $L_0$에서 측정한 거리 값 $L$의 차를 구해 $r = L - L_0$로 산출합니다. 그 후, $x = r \\cos{⁡\\theta}, y=r \\sin{\\theta}$에 따라 직교 좌표로 변환한 다음, SD 카드에 $(x,y,z)$ 형식으로 점을 저장합니다. 한 바퀴 회전을 마치고 나면 미리 설정한 z-해상도에 따라 거리 센서 carriage를 z 방향으로 일정 거리 증가시킵니다. 이를 반복하여 모든 z-slices에 대해 스캔을 수행하여 pointcloud를 저장합니다. SD 카드에 저장된 pointcloud 데이터는 PC에서 Python 프로그램을 실행하여 시각화할 수 있고, meshlab 소프트웨어를 사용해 mesh로 변환하여 stl 파일로 저장할 수 있습니다.5.2 설정값 변화에 따른 스캔 퀄리티 비교θ-해상도와 z-해상도를 LOW, MEDIUM, HIGH로 변화시킬 때의 스캔 퀄리티를 비교하였습니다.5.2.1 두꺼비 모형(a) Frog original image(b) LOW (c) MEDIUM (d) HIGH resolution pointcloud, (e) LOW (f) MEDIUM (g) HIGH resolution mesh높은 해상도로 갈수록 스캔된 점의 개수가 증가하여 더욱 세밀한 부분까지 묘사되었습니다. 특히 입 쪽에 뚫려있는 구멍과 등의 돌기를 관찰하면 높은 해상도일수록 뚜렷하게 드러남을 확인할 수 있습니다.5.2.2 플라스틱 컵(a) Plastic cup original image(b) LOW (c) MEDIUM (d) HIGH resolution pointcloud, (e) LOW (f) MEDIUM (g) HIGH resolution mesh컵의 손잡이와 컵 표면 중간의 굴곡을 보면 높은 해상도일수록 뚜렷하게 드러남을 확인할 수 있습니다. 그러나 HIGH 해상도의 경우 너무 많은 점을 스캔하여 매끄러운 컵의 표면이 오히려 울퉁불퉁하게 모델링되었습니다.5.3 그 외 스캔 결과 정리5.3.1 다이캐스트 자동차 모형Diecast model car (a) original image (b) MEDIUM resolution pointcloud (c) MEDIUM resolution meshMEDIUM 해상도로 스캔하였습니다. 자동차의 하체 부분의 경우 타이어와 차체의 바디가 잘 스캔되었으나, 상체 부분은 형상이 왜곡되었습니다. 이는 투명한 재질의 창문에 의해 적외선이 차체 내부로 투과되어 발생한 오류라고 판단됩니다.5.3.2 납땜보조제 케이스Paste case (a) original image (b) LOW resolution pointcloud (c) LOW resolution meshLOW 해상도로 스캔하였습니다. 기울어진 뚜껑과 중간의 홈 형상이 비교적 잘 스캔된 것을 확인할 수 있습니다.5.3.3 알람 시계Alarm clock (a) original image (b) MEDIUM resolution pointcloud (c) MEDIUM resolution meshMEDIUM 해상도로 스캔하였습니다. 앞부분의 액정이 왜곡되어 스캔되었는데, 이는 자동차 모형과 마찬가지로 적외선이 액정으로 투과되어 발생한 것으로 추정됩니다.5.3.4 약통Medicine case (a) original image (b) HIGH resolution pointcloud (c) HIGH resolution meshHIGH 해상도로 스캔하되, 스캔 시간을 단축시키기 위해 의도적으로 z=0 지점을 위로 조정하였습니다. 전체적으로 잘 스캔되었으나, 옆면 중간에 홈이 생성되었습니다. 이는 라벨 스티커의 영향으로 추정됩니다.정리하면, 저가형 IR 센서를 사용했음에도 불구하고 비교적 복잡한 형상도 잘 스캔함을 확인할 수 있었습니다. 복잡한 형상이 많은 물체일수록 높은 해상도로 스캔하는 것이 유리하지만, 표면이 매끄럽고 단순한 형상인 물체는 낮은 해상도로 스캔하는 것이 오히려 유리하였습니다. 또한 물체에 다양한 재질이 섞여있거나 투명한 재질이 존재하는 경우 스캔 퀄리티가 저하된다는 아쉬운 점이 존재하였습니다. 이는 고가의 거리 측정 센서를 사용하거나, 데이터 후처리를 심도있게 하여 해결할 수 있을 것으로 예상됩니다." }, { "title": "[3D scanner] 3D scanner 제작기 - 3", "url": "/posts/3D_scanner_3/", "categories": "Making, 3D scanner", "tags": "3D scanner, software, TOF, laser, eccentric, Laplace smoothing, meshlab, PROGMEM, F() macro, flow chart", "date": "2023-04-09 00:00:00 +0900", "snippet": " 관련 포스팅 [3D scanner] 3D scanner 제작기 - 1 [3D scanner] 3D scanner 제작기 - 2 4. 소프트웨어 개발 과정4.1 문제점 및 해결 과정소프트웨어 개발 중 맞닥뜨린 문제점과 그 해결 과정을 위주로 서술하였습니다.4.1.1 IR 거리 센서와 레이저 TOF 거리 센서 비교 구분 GP2Y0A21YK0F IR 센서 VL5310X Laser TOF 센서 통신 아날로그 전압 신호 I2C 제어핀 아날로그 입력 핀 A4(SCL), A5(SDA) 측정 원리 적외선이 물체에서 반사된 지점에 따라 수신부에 도달하는 지점의 차이를 이용해 측정 레이저 빛이 발사되고 물체에 반사되어 수신될 때까지 시간을 측정 VL5310X 레이저 TOF 거리 센서 역시 10000원 대의 저렴한 가격으로 구매할 수 있는 거리 측정 센서입니다. 두 센서 중 어느 센서를 사용할지 결정하기 위해 사진과 같은 멀티미터를 한 slice 스캔하여 그 결과를 비교해 보았습니다. 별도의 데이터 후처리는 수행하지 않았습니다.Multimeter with edgeScan result w/ IR sensorScan result w/ Laser TOF sensorIR 거리 센서 측정 결과를 보면 적외선의 입사각이 10도보다 작은 네 부분에서 규칙적으로 펄스가 관찰되었습니다. 반면 레이저 TOF 거리 센서의 측정 결과에서는 전체적인 정밀도는 높지만, 몸체가 얇게 왜곡되었습니다.실험 결과 멀티미터의 주황색 케이스 부분의 엣지 부분이 원인으로 파악되었습니다. 직진성이 강한 레이저의 특성 상, 물체 표면의 방향이 레이저와 수직을 이루지 못할 경우 다른 곳으로 반사되므로 불규칙한 형상을 스캔할 시 정밀도가 크게 하락하였습니다.이러한 제약을 감수할 수는 없으므로, 본 3D scanner 프로젝트에서는 IR 거리 센서를 사용하였습니다. 원래 레이저 TOF 거리 센서의 측정 범위는 2m 임을 생각하면 3mm 남짓의 오차는 센서 가격 대비 작은 편에 속하므로 이 센서는 추후에 자작 Lidar 프로젝트 때 사용해볼 계획입니다.4.1.2 IR 거리 센서의 측정 오차 원인 분석IR 거리 센서를 사용하기로 결심했지만, 발견된 오차를 무시할 수는 없습니다. 규칙적으로 발생하는 펄스를 좀 더 정량적으로 측정하기 위해 rotating plate 위에 회전축을 지나는 평면을 붙인 다음 180도 스캔하여 입사각에 따른 오차를 측정해보았습니다. 그 결과 다음과 같이 도출되었습니다.Incidence angle vs IR sensor distance error이제 예상 오차 가설과 검증 과정을 정리해봅시다. 이 과정이 정말 오래 걸렸고 저에게 극심한 스트레스를 주었지만, 사실 결과는 허무하게 끝났습니다…1) 회전 중심에서 거리에 따른 지점의 선속도 차이회전 중심에서 먼 지점일수록 선속도가 커지므로 잔상 효과를 크게 받게 됩니다. 가장 먼저 떠올린 가설이었지만, 물체의 회전 방향을 바꾸어 측정하여도 펄스의 방향은 유지되었기 때문에 본 가설은 기각되었습니다.2) IR 거리 센서의 방향과 회전 중심 사이의 편심Eccentric model of IR distance sensor센서랑 모터 조립 시 오차가 발생하여 IR 거리 센서와 회전 중심 사이에 편심이 발생할 가능성이 존재합니다. 이를 이론적으로 분석해봅시다. 위의 그림과 같이 원래 빨간색으로 표시한 평면을 파란색으로 표시한 회전 중심을 지나는 센서로 측정해야하지만, 편심 s가 발생한 노란색 센서로 측정했다고 가정해봅시다.빨간색으로 표시한 직선의 방정식은 극좌표계에서 다음과 같이 기술할 수 있습니다.\\[r(\\theta) = \\frac{d}{\\cos{\\theta}}\\]편심 $s$가 있는 센서로 평면을 스캔할 때의 측정 결과를 $r’(\\theta)$라 합시다. 노란색 삼각형과 초록색으로 표시한 각도 $\\phi$를 보면 다음이 성립합니다.\\[\\begin{align*}\\sin{\\phi} &amp;amp;= \\frac{s}{r(\\theta - \\phi)} \\\\&amp;amp;= \\frac{s}{d} \\cos{(\\theta - \\phi)} \\\\&amp;amp;= \\frac{s}{d} (\\cos{\\theta} \\cos{\\phi} + \\sin{\\theta} \\sin{\\phi})\\end{align*}\\]\\[1 = \\frac{s}{d} \\left( \\frac{\\cos{\\theta}}{\\tan{\\phi}} + \\sin{\\theta} \\right)\\]\\[\\therefore \\tan{\\phi} = \\frac{s \\cos{\\theta}}{d - s \\sin {\\theta}}\\]따라서 $r’(\\theta)$는 다음과 같이 유도됩니다.\\[\\tan{\\phi} = \\frac{s}{r&#39;(\\theta)} = \\frac{s \\cos{\\theta}}{d - s \\sin{\\theta}}\\]\\[\\therefore r&#39;(\\theta) = \\frac{d - s \\sin{\\theta}}{\\cos{\\theta}}\\]Eccentric model plot위 그래프에서 빨간색 그래프는 원래 평면, 초록색 그래프는 편심 모델에 따라 왜곡된 측정 결과를 나타냅니다. 편심 수치를 과장하여 $d=20mm$, $s=5mm$를 대입하였음에도 불구하고 펄스를 닮지 않은걸 확인할 수 있습니다. 따라서 본 가설 역시 기각되었습니다.3) 센서와 표면 재질에 따른 고유 특성다른 가설은 없을지 머리를 싸매고 며칠간 고민했지만, 뾰족한 생각이 들지 않았습니다. 물체를 바꾸어가며 측정해보았지만 재질에 따라 정도의 차이만 있을 뿐 규칙적으로 펄스가 발생하였기 때문입니다. 결국 GP2Y0A21YK0F IR 센서의 고유한 특성으로 인정하였습니다. 이하 내용부터는 거리 센서의 오차를 입사각 $\\phi_{inc}$에 대한 함수 $e(\\phi_{inc})$로 두어 수치적인 모델로 해결하는 과정을 서술하였습니다.4.1.3 IR 거리 센서의 측정 오차 해결 과정1) 오차 모델 수립 및 모델 기반 해결Incidence angle in ploar coordinate물체의 실제 표면을 $r(\\theta)$라고 하면, 거리 벡터( $\\vec{r} = \\vec{OP}$ )와 물체의 표면이 이루는 각도 $\\frac{\\pi}{2} - \\phi_{inc}$는 다음과 같이 구할 수 있습니다.\\[\\tan{ \\left( \\frac {\\pi}{2} - {\\phi}_{inc} \\right) } = \\frac{r(\\theta)}{\\frac{\\partial r}{\\partial \\theta} (\\theta)}\\]\\[\\phi_{inc} = \\frac{\\pi}{2} - \\rm{atan2} \\left( r(\\theta), \\frac{\\partial r}{\\partial \\theta}(\\theta) \\right)\\]따라서 오차 모델을 다음과 같이 수립할 수 있습니다.\\[\\begin{align*}r&#39;(\\theta)&amp;amp;= r(\\theta) + e \\left( \\frac{\\pi}{2} - \\rm{atan2} \\left( r(\\theta), \\frac{\\partial r}{\\partial \\theta}(\\theta) \\right) \\right) \\\\&amp;amp;= r(\\theta) + f \\left( \\theta; r, \\frac{\\partial r}{\\partial \\theta} \\right)\\end{align*}\\]이는 주어진 $f$와 측정한 $r’(\\theta)$를 바탕으로 $r(\\theta)$를 찾는 전형적인 inverse problem입니다. 그러나 오차 함수가 비선형 함수이고, 함수 $f$가 $r, \\frac{\\partial r}{\\partial \\theta}$에 동시에 의존하기 때문에 이를 풀기는 쉽지 않습니다. 저 역시 이를 해결할 수치적인 방법을 고안하지 못하였기 때문에, 아쉽게도 모델 기반 해결책은 기각하였습니다.2) 모델 프리 해결결국 외부 소프트웨어의 도움을 빌렸습니다. “Meshlab”이라는 오픈 소스 3D mesh 툴을 이용하여 pointcloud를 mesh로 변환한 다음, 적용할 수 있는 surface smoothing 알고리즘을 비교하였습니다. 그 결과, Laplace smoothing 알고리즘이 가장 적절하게 펄스 노이즈를 제거하는걸 확인했습니다. Meshlab 프로그램과 Laplace smoothing에 대한 설명은 다음을 참고하시면 좋을 것 같습니다. 참고 자료 MeshLab Laplacian mesh processing 4.1.4 SRAM 메모리 부족 문제 및 해결아두이노 UNO를 사용하기로 결심한 후 메모리 용량이 부족하지 않을까 고민했는데… 걱정이 현실이 되었습니다. 개발 후반후에 접어들면서 아두이노 IDE 상에서 메모리 부족 경고가 뜨더니 결국 컴파일해도 작동을 하지 않더라고요. 아두이노 UNO의 SRAM 용량은 2048Bytes밖에 되지 않기 때문에 항상 주의를 기울여야 합니다. 저는 다음과 같은 해결책을 적용하여 이러한 문제를 해결할 수 있었습니다. Hyperparameter의 경우 PROGMEM 명령어를 사용하여 flash 메모리에 데이터를 저장합니다. 대표적으로 속도와 정밀도 세팅값, LCD에 출력할 custom character가 있습니다. 문자열을 함수의 인수로 대입할 시 F() macro를 사용하여 flash 메모리에 저장합니다. 변수의 범위를 고려하여 알맞은 변수 type를 선택합니다. 예를 들어 속도나 해상도 세팅은 3~4가지가 있으므로 byte 변수로 저장하고, flag 변수는 0, 1만 가지므로 bool, z slices 값은 기하학적으로 1000 이하이기 때문에 uint16_t로 저장합니다. 아두이노 IDE에서 지역 변수가 사용하는 메모리는 추적하지 못하기 때문에 메모리 사용량을 가늠하기 쉽도록 지역 변수를 최소화하였습니다. 또한 일부 변수는 재사용하였습니다. 참고 자료 PROGMEM, F() macro 4.2 class 설명각 class별 역할과 기능을 설명합니다.4.2.1 hyperparameter.hNEMA17의 한 회전 당 스텝 수 200, A4988에 설정해둔 분주 수 16, IR 거리 센서에서 plate 중심까지 거리 158mm 등 3D scanner 기본 세팅 값이 저장되어있습니다. PROGMEM을 이용하여 flash 메모리에 저장하는 테크닉이 여기에 많이 적용되었습니다.4.2.2 lcd.h &amp;amp; lcd.cpp아두이노 LCD I2C 모듈을 제어하는 라이브러리 LiquidCrystal_I2C를 상속하여 작성하였습니다. 특정 줄의 문장을 스크롤하는 custom_scroll()와 막대 그래프를 출력하는 bar_graph(), 아두이노 flash 메모리에 저장된 특수 문자를 불러오는 createChar_P() 함수가 구현되어 있습니다.4.2.3 A4988_stepper.h &amp;amp; A4988_stepper.cppA4988 모터 드라이버로 스텝 모터를 제어하는 클래스입니다. A4988은 pulse의 duration을 통해 모터의 속도를 조절하는데, 이러한 duration을 설정하는 set_period() 함수가 구현되어 있습니다. 또한 모터의 방향을 조절하는 set_dir(), 입력한 스텝만큼 회전하는 rotate_step() 등의 함수가 구현되어 있습니다.4.2.4 button.h &amp;amp; button.cppRotary encoder의 버튼과 limit switch를 제어하는 클래스입니다. 버튼이 눌렸는지 확인하는 is_pushed() 함수와 버튼이 한 번 클릭되었는지 더블 클릭되었는지 판단하는 is_singled_or_double_ clicked() 함수가 구현되어 있습니다.4.2.5 distance_sensor.h &amp;amp; distance_sensor.cppSharp 사의 GP2Y0A21YK0F IR 거리 센서로 측정한 거리를 mm 단위로 반환하는 get_distance() 함수가 구현되어 있습니다. 거리 측정 시 정밀도를 향상하기 위해 5회 측정한 평균값을 반환합니다.4.3 3D scanning 흐름도3D scanning flow chart각 단계는 rotary encoder 버튼을 1번 클릭 시 다음으로 넘어갈 수 있고, 더블 클릭 시 이전 화면으로 되돌아갑니다. LCD 화면을 통해 세팅 화면을 출력합니다.4.3.1 시작 화면SD 카드를 장착했는지 확인합니다. 확인은 5초마다 수행됩니다. SD 카드가 장착되어 있는 채로 버튼을 클릭하면 다음 화면으로 이동합니다.4.3.2 모터 속도 세팅Rotating plate의 회전 속도를 결정합니다. 회전 속도가 빨라질 시 전체 scanning 시간은 감소하지만, IR 거리 센서로 물체의 표면을 스캔할 때 잔상 효과가 발생하고 물체의 진동이 커져 정확도가 감소합니다. Rotary encoder를 시계, 반시계 방향으로 회전하여 SLOW, MEDIUM, FAST, VERY FAST 4 단계로 조절할 수 있습니다.4.3.3 θ-해상도 세팅한 회전에 측정하는 점의 수를 결정합니다. LOW, MEDIUM, HIGH 3단계로 조절할 수 있으며, LOW의 경우 100pts/rev, MEDIUM은 200pts/rev, HIGH는 400pts/rev의 해상도로 스캔합니다.4.3.4 z-해상도 세팅물체를 스캔하는 z 방향 간격을 결정합니다. LOW, MEDIUM, HIGH 3단계로 조절할 수 있으며, LOW의 경우 3mm 간격, MEDIUM은 2mm 간격, HIGH는 1mm 간격으로 스캔합니다.4.3.5 원점 세팅거리 센서 carriage를 limit switch 위치까지 내렸다가 사용자가 z=0으로 둘 높이를 설정하는 단계입니다. Rotary encoder를 시계, 반시계 방향으로 돌려 z=0 위치를 조절할 수 있습니다.4.3.6 물체 확인물체를 plate 위에 올렸는지 확인하는 단계입니다.4.3.7 Pre-scanning사용자가 설정한 z=0 지점부터 물체의 최고점까지 높이를 측정하여 z-slices 개수를 계산합니다. 거리 센서 carriage를 z=0부터 일정 거리만큼 올리면서 측정값과 미리 정해둔 최대 거리값을 비교합니다. 최대 거리값보다 작게 측정될 경우 물체가 존재한다는 의미이므로 다시 carriage를 1 slice 상승시킵니다. 최대 거리값보다 크게 측정될 경우 이 각도에는 물체가 없지만 다른 각도에는 물체가 존재할 수 있으므로 plate를 1바퀴 회전하면서 계속 비교를 수행합니다. 모든 각도에서 물체가 존재하지 않다고 판단될 경우 pre-scanning을 종료하고 z-slices 개수를 LCD에 출력합니다. 이는 scanning 단계에서 진행도를 계산하는데 사용됩니다.4.3.8 Scanning스캔을 시작하기 전에 SD 카드가 슬롯에 있는지 다시 한번 확인하고, SD 카드가 존재하지 않으면 시작 화면으로 돌아갑니다. SD 카드가 정상적으로 존재할 경우 SD 카드의 “(root 폴더에 있는 파일 개수)+1.txt”의 이름으로 파일을 만들고 스캔한 점의 좌표를 기록합니다. 스캔 과정에서, 스캔을 완료한 z-slices 개수와 전체 z-slices 개수로부터 진행도를 계산하여 LCD에 막대 그래프로 출력합니다. 스캔이 완료되면 파일 쓰기를 종료하고 시작 화면으로 돌아갑니다.4.3.9 데이터 후처리Python open3d 라이브러리를 사용하여 pointcloud 데이터를 시각화할 수 있습니다. Meshlab 프로그램을 사용하여 pointcloud를 mesh로 변환할 수 있습니다." }, { "title": "[3D scanner] 3D scanner 제작기 - 2", "url": "/posts/3D_scanner_2/", "categories": "Making, 3D scanner", "tags": "3D scanner, electric device, Solidworks, arduino, IR, stepper motor, motor driver, A4988, L298n, SD card, rotary encoder, lcd", "date": "2023-04-08 00:00:00 +0900", "snippet": " 관련 포스팅 [3D scanner] 3D scanner 제작기 - 1 3. 전자 회로 제작 과정3.1 개발 보드 선택 구분 Arduino UNO Raspberry Pi 4 언어 C++ Python 멀티태스킹 X O 입출력 전압 0~5V 0~3.3V ADC O(A0~A5 핀) X 리소스 부족 충분 라즈베리파이의 경우 0~3.3V 범위의 GPIO를 사용하기 때문에 5V 전압에 맞춰져있는 디바이스를 사용할 시 변환 작업이 필요합니다. 라즈베리파이에는 내장 ADC(Analog to Digital Converter)가 없기 때문에 아날로그 센서를 사용할 때 별도의 ADC 회로를 구축해 주어야 합니다. 아두이노에서 사용할 수 있는 SRAM은 2048bytes로 개발 시 리소스가 부족할 우려가 있습니다. 본 프로젝트의 경우 모든 동작이 순차적으로 수행되는 상황이 많아, 멀티쓰레드를 사용하지 않고도 어렵지 않게 구현할 수 있을 것으로 예상하였습니다. 또한 SD 카드라는 별도의 저장 장치에 pointcloud를 저장할 예정이므로, 리소스 부족 문제도 해결할 수 있을 것으로 생각하였습니다. 이에 본 3D scanner에서는 회로를 단순화하기 용이한 아두이노 UNO를 개발 보드로 선정하였습니다. VScode에 아두이노 extension을 설치하여 개발 환경을 구축하였습니다.3.2 IR 거리 센서 거리-전압 관계 fittingGP2Y0A21YK0F distance-voltage relation본 프로젝트에서는 Sharp 사의 GP2Y0A21YK0F IR 거리 센서를 사용하였습니다. 데이터 시트를 참고하여 유효한 범위(L &amp;gt; 50mm)에서 거리-전압 관계를 log-log fitting하여 다음과 같은 관계식을 얻었습니다.\\[L=299.88 \\cdot V^{-1.173}\\]3.3 모터 드라이버 및 스텝 모터 선택Comparison between L298n and A4988 motor driver 구분 L298n A4988 배선 단순 복잡 제어핀 4개 2개 진동&amp;amp;소음 큼 작음 비고 - 분주 조절 가능 Comparison between applied motion 4017-875 and 42shdc3025-24b stepper motor 구분 Applied motion 4017-875 42shdc3025-24b Rated voltage 3.5V 3.96V Rated current 0.35A 0.9A Rotating plate를 5t 아크릴 원판으로 제작하였기 때문에 회전 관성 모멘트가 굉장히 큽니다. 이로 인해 θ-방향으로 plate를 회전할 시 많은 토크가 요구되어, 대부분의 스텝 모터에서 탈조 현상이 발생해버렸습니다. 시행착오를 거친 결과, 소비 전력이 큰 42shdc3025-24b 스텝 모터와 분주 조절이 가능한 A4988을 조합하고, A4988을 16분주로 설정하여 rotating plate를 작동시킬 수 있었습니다. z-방향 이동에 사용하는 리드 스크류 회전에는 큰 토크가 요구되지 않으므로, 상대적으로 배선이 단순한 L298n 모터 드라이버와 남은 스텝 모터(4017-875)를 사용하였습니다.3.4 SD card reader 자작원래는 PC와 시리얼 통신으로 바로 pointcloud 정보를 전송하려고 했으나, 계획을 변경하여 SD 카드를 사용하기로 하였습니다. 인터넷을 뒤져보니 나름 구조가 단순한거 같아 직접 자작해보기로 하였습니다. SD 카드 리더는 아래의 글을 참고하여 제작하였습니다. 참고 자료 Cheap Arduino SD Card Reader DIY SD card reader SD 카드 어댑터에 핀헤더를 납땜하여 회로를 구성하고 마이크로 SD 카드를 삽입하여 데이터를 읽고 씁니다. SD 카드 리더는 SPI 통신에 사용되는 4개의 핀(CS, MOSI, MISO, SCK)과 3.3V, GND 총 6개의 핀으로 구성되어 있습니다. 핀맵에 맞추어 연결해줍니다. SD 카드는 3.3V 입출력을 사용하므로 voltage divider 회로를 사용하여 아두이노의 5V 입출력 신호를 3.3V에 맞춰 변환해줍니다. (저는 1kohm 저항과 470ohm 저항을 사용하였습니다.) 이렇게 만든 DIY SD card reader는 매우 잘 동작했지만, 이후 순간접착제로 붙이는 과정에서 SD 카드 인식부가 녹아 합선이 발생하였습니다. 결국 급하게 SD card reader를 인터넷에서 구입하여 사용하였습니다. (2500원밖에 안합니다. 그냥 살걸…)3.5 아두이노 proto-shield 회로 설계 및 납땜Prototype circuit그림처럼 브레드보드에 모든 디바이스를 연결하고 실험해보았습니다. 잘 되는걸 확인한 다음 바로 납땜을 시작하였습니다.Proto-shield circuit design (front)Proto-shield circuit design (back)Proto-shield circuit result (front)Proto-shield circuit result (back)아두이노 보드 위에 장착할 수 있는 특수한 기능의 회로를 shield라고 합니다. 그중에는 사용자가 직접 회로를 납땜하여 꾸밀 수 있는 proto-shield라는 것이 있습니다. 저는 proto-shield에 3D scanner 시스템을 납땜하여 사용하였습니다. F/F 점퍼선의 한쪽 끝을 몰렉스 커넥터로 바꾼 케이블을 사용하여 디바이스와 proto-shield 사이 체결을 용이하게 하였습니다. 이를 위해 proto-shield에는 다양한 몰렉스 커넥터를 납땜하여 아두이노 입출력 핀과 연결해주었습니다. A4988 모터 드라이버는 proto-shield 중앙에 핀헤더를 납땜한 후 연결하였습니다.3.6 회로 케이스 설계 및 제작Circuit case modeling planCircuit case modeling floor planCircuit case assembly그림과 같이 3D scanner에 들어가는 전자 부품과 아두이노, 전원부를 담는 케이스를 설계하고 제작하였습니다. 3t 아크릴을 커팅하여 제작하였습니다.3.7 전자 회로 완성3D scanner circuit diagram3D scanner electric device system Arduino UNO pin Electric device D2, D3 Rotary encoder(encoder) D4, D5, D6, D7 L298n motor driver D8, D9 A4988 motor drivier D10, D11, D12, D13 SD card reader A0 Rotary encoder(button) A1 IR distance sensor A2 Limit SW A4, A5 LCD I2C module 3.7.1 아두이노 UNO USB type-B 포트를 통해 PC와 시리얼 통신을 수행합니다. SMPS의 12V 소스를 전원으로 작동합니다. 몰렉스 커넥터와 A4988 모터 드라이버 전용 핀헤더 소켓을 납땜한 proto-shield를 장착하였습니다.3.7.2 전원부 220V IEC power cable을 꽂을 수 있는 connector와 전원 스위치, 5W 퓨즈로 구성되어 있습니다. Power connector는 SMPS의 220V AC 전원 입력부와 연결되며, SMPS의 12V 출력은 아두이노 UNO와 A4988, L298n 모터 드라이버에 인가됩니다.3.7.3 Rotary encoder 사용자가 3D scanner를 제어할 수 있도록 입력 장치 역할을 수행합니다. 시계, 반시계 방향으로 encoder를 돌리는 입력과 버튼을 클릭하는 입력을 받을 수 있습니다. 아두이노 UNO에 배정되어있는 인터럽트 핀(D2, D3)을 사용하여 encoder 입력을 즉각적으로 처리할 수 있도록 하였습니다. 남은 디지털 핀이 없어, 버튼 입력은 A0 핀에 할당하였습니다. 별도의 pull-up 회로를 꾸미지 않았고, 아두이노 내부 pull-up 저항을 사용하였습니다.3.7.4 L298n motor driver &amp;amp; z-dir stepper motor z-방향 stepper motor를 제어하기 위해 L298n 모터 드라이버를 사용하였습니다. 4개의 디지털 핀 D4, D5, D6, D7을 사용하여 모터 드라이버에 입력 신호를 인가합니다.3.7.5 A4988 motor driver &amp;amp; θ-dir stepper motor A4988 모터 드라이버를 16 분주 모드로 사용하여 스텝 모터가 속도는 느리지만 큰 토크를 낼 수 있도록 하였습니다. 2개의 디지털 핀 D8, D9를 사용합니다. 하나는 모터의 회전 방향을 HIGH, LOW로 조절하고, 하나는 PWM의 duration을 조절하여 모터의 회전 속도를 조절합니다.3.7.6 SD card reader 3D 스캔 결과인 pointcloud 파일을 SD 카드에 저장합니다. 아두이노 UNO에 배정되어있는 SPI 통신 핀(D10-Chip Select, D11-MOSI, D12-MISO, D13-SCK)을 사용하였습니다.3.7.7 IR distance sensor Sharp 사의 GP2Y0A21YK0F IR 거리 센서를 사용하였습니다. 거리 센서 carriage에서 물체의 표면까지의 거리를 측정하여 원통 좌표계에서의 r 값을 산출합니다. 0~5V 사이의 전압을 출력하는 아날로그 센서 값을 읽기 위해 ADC(Analog to Digital Converter)가 내장되어있는 A1핀에 할당하였습니다.3.7.8 Limit SW 거리 센서 carriage가 프로파일 프레임에 닿는지 여부를 판단하는 역할을 수행합니다. 자체 회로에 pull-up 저항이 존재합니다. 남은 디지털 핀이 없어 A2 핀에 할당하였지만, 디지털 입력으로 받습니다.3.7.9 LCD I2C module 16×2 사이즈의 display를 사용하여 사용자가 3D 스캐너 상태를 파악할 수 있도록 합니다. 가용할 수 있는 핀이 부족하여 I2C 모듈이 장착된 LCD를 사용하였습니다. 아두이노 UNO에 배정되어있는 I2C 통신 핀(A4-SCL, A5-SDA)을 사용하였습니다." }, { "title": "[3D scanner] 3D scanner 제작기 - 1", "url": "/posts/3D_scanner_1/", "categories": "Making, 3D scanner", "tags": "3D scanner, hardware, Solidworks, modeling", "date": "2023-04-07 00:00:00 +0900", "snippet": "6월에 군대 입대가 예정되어있어 휴학 중에 있습니다.며칠 동안은 자유를 만끽했지만 이내 몸이 근질거리더라고요.무슨 일을 벌일까 고민을 하다가, 저를 기계공학부로 오도록 만들어준 making을 오랜만에 해보기로 했습니다.초, 중학교 때 취미 삼아서 과학상자, 아두이노 등을 많이 하면서기계/로봇공학자가 되고 싶다라고 꿈을 꿔왔거든요.이번에는 대학생 수준에 맞춰 무에서 유를 창조해보도록 했습니다.제가 자주 찾아보는 “Mr Innovative” 이라는 채널입니다.아두이노로 간단하면서 참신한 making을 올리는 채널인데,그 중 3D 스캐너 영상이 눈에 들어왔습니다.구조도 단순하고 직관적이며 3D 스캔 퀄리티도 나쁘지 않게 나오는걸 확인했습니다.마침 집에 Nema 17 스텝 모터도 많고 거리 센서도 있는지라 만들어보기로 했습니다.Pointcloud 데이터 처리 방법도 익힐 수 있는 좋은 기회라고 생각이 듭니다.1. 부품 공수1.1 집에서 공수집에 굴러다니거나 기존에 만들었던 프로젝트의 하드웨어를 분해해서 얻은 부품들입니다. SMPS (LRS-100-12) NEMA 17 스텝 모터 3개 Applied motion 4017-875 2개 42shdc3025-24b 1개 A4988 모터 드라이버 2개 L298n 모터 드라이버 2개 LCD 16x2 I2C 모듈 1개 20mm 프로파일1.2 싸이피아 (Scipia)3D 프린터 부품 전문 온라인 쇼핑몰입니다. 5mm-8mm 커플링 8mm 리드 스크류 + 전용 리드 스크류 너트 KP08 유니트 베어링 SK8 샤프트 고정 클램프1.3 미스미 (Misumi) 8mm 연마봉 2탭 세트칼라1.4 청계천 세운상가세운상가에 가서 직접 하드웨어 부품을 구입하였습니다. 고무발 LMK8UU 사각플랜지형 리니어볼부쉬 프로파일 부속품 M4, M5 스프링 너트 ㄱ자 블라인드 브라켓 Sharp GP2Y0A21YK0F IR 거리 센서 HW-080 로터리 엔코더 IEC 파워 케이블, 커넥터, 5W 퓨즈1.5 디바이스마트 (Devicemart) 점퍼 케이블 F/F A4988 모터 드라이버(여분) 몰렉스 커넥터 2, 3, 4, 5, 6 pin, 클림프 수축 튜브 SD card reader module2. 하드웨어 제작 과정2.1 하드웨어 모델링Solidworks를 이용하여 모델링하였습니다.기본적인 골격은 20mm 프로파일로 구성하되, 3D 프린터와 레이저 커터로 부품을 추가로 가공하였습니다.3D scanner modeling plan3D scanner modeling floor plan3D scanner modeling front plan2.2 3D 프린팅 &amp;amp; 레이저 커팅3D 프린터와 레이저 커터는 서울대학교 해동 아이디어 팩토리에서 사용하였습니다.Laser cutting w/ VLS 6.60 3D 프린터 출력 (신도리코 3D WOX DP200) IR 거리 센서 carriage 스크류, 연마봉 고정 장치 아크릴 레이저 커팅 (유니버셜 레이저 VLS 6.60) 스텝 모터-프로파일 고정 브라켓(3t) Rotating plate(5t) 2.3 하드웨어 조립3D scanner hardware assembly (LCK를 보며 :D) 그라인더를 사용하여 치수에 맞게 프로파일을 커팅하였습니다. 20mm 프로파일 스프링 너트에 고무발을 체결할 수 있도록 고정 볼트를 M5로 개조하였습니다. Rotating plate에서 볼트 머리를 숨기기 위해, 아크릴에 M4 카운터보어로 가공하였습니다.2.4 하드웨어 완성3D scanner hardware system 기본적으로는 20mm×20mm 프로파일을 골격으로 제작하였습니다. 2개의 가로 프로파일 사이에 z-방향 스텝 모터와 θ-방향 스텝 모터를 브라켓을 이용하여 고정하였습니다. θ-방향 스텝 모터에 세트 칼라를 사용하여 rotating plate를 연결하였습니다. z-방향 스텝 모터에는 커플러를 거쳐 리드 스크류와 연결됩니다. 리드 스크류와 리드 스크류 너트를 이용하여 z-방향 스텝 모터의 회전 운동을 거리 센서 carriage의 선형 운동으로 변환합니다. 거리 센서 carriage는 IR 거리 센서와 리드 스크류 너트, 리니어 볼부쉬가 설치되어 있습니다. 거리 센서 carriage가 흔들리지 않도록 2개의 연마봉이 프로파일과 클램프를 통해 고정되어있고, 리니어 볼부쉬가 각 연마봉을 통과하여 지나다니게 제작되었습니다. 연마봉과 리드 스크류는 고정 장치를 통해 상단에서 고정됩니다. 스텝 모터의 진동으로 인해 3D 스캐너 본체가 흔들리지 않도록 고무발을 장착하였습니다." }, { "title": "[Lie group] SE(3) group 정리", "url": "/posts/SE3_group/", "categories": "SLAM", "tags": "lie group, SE(3), euclidean, screw motion, exponential, logarithm, twist, spatial velocity, adjoint", "date": "2022-09-19 00:00:00 +0900", "snippet": "SO(3) group에 이어서 SE(3) group에 대해 정리해본다. 관련 포스팅 [Lie group] SO(3) group 정리 reference textbookLynch, Kevin M., and Frank C. Park. Modern robotics. Cambridge University Press, 2017.1. Special Euclidean Group, SE(3)$SE(3)$ group은 강체의 3차원 상 운동을 기술한다. 강체의 운동은 크게 회전 운동과 병진(평행) 운동으로 나눌 수 있다는 점에 착안하면, 회전 운동은 $SO(3)$ matrix $R$으로, 평행 이동은 3차원 벡터 $p \\in \\mathbb{R}^3$로 표현할 수 있다. 이를 종합하여 $T \\in SE(3)$은 $4 \\times 4$ matrix로 다음과 같이 표현된다.\\[T = \\begin{bmatrix} R &amp;amp; p \\\\ 0 &amp;amp; 1\\end{bmatrix}=\\begin{bmatrix} r_{11} &amp;amp; r_{12} &amp;amp; r_{13} &amp;amp; p_1\\\\r_{21} &amp;amp; r_{22} &amp;amp; r_{23} &amp;amp; p_2\\\\r_{31} &amp;amp; r_{32} &amp;amp; r_{33} &amp;amp; p_3\\\\0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1\\end{bmatrix}\\]이때 $p$는 변환 전 좌표계에서 기술한 병진 운동 성분이라는 점에 유의하여야 한다. $R$에 의해 회전된 좌표계에서 기술한 방향이 아니다.2. SE(3) 성질SE(3) 행렬이 만족하는 성질을 정리하면 다음과 같다. {a} $\\rightarrow$ {b} 좌표계로 변환하고 {b} $\\rightarrow$ {c} 좌표계로 변환한 결과는 {a} $\\rightarrow$ {c} 좌표계로 변환한 결과와 동일하다.\\[T_{ab} T_{bc} = \\begin{bmatrix} R_{ab} &amp;amp; p_{ab} \\\\ 0 &amp;amp; 1\\end{bmatrix} \\begin{bmatrix} R_{bc} &amp;amp; p_{bc} \\\\ 0 &amp;amp; 1\\end{bmatrix} = \\begin{bmatrix} R_{ab} R_{bc} &amp;amp; R_{ab} p_{bc} + p_{ab} \\\\ 0 &amp;amp; 1\\end{bmatrix} = \\begin{bmatrix} R_{ac} &amp;amp; p_{ac} \\\\ 0 &amp;amp; 1\\end{bmatrix} = T_{ac}\\] 항상 역변환이 존재하므로, 역행렬이 존재한다.\\[\\begin{bmatrix} R &amp;amp; p \\\\ 0 &amp;amp; 1\\end{bmatrix}^{-1} = \\begin{bmatrix} R^T &amp;amp; -R^T p \\\\ 0 &amp;amp; 1\\end{bmatrix}, \\;\\; T_{ab}^{-1} = T_{ba}\\] 공간 상의 점 $Q$를 {a} 좌표계, {b} 좌표계에서 기술한 위치 벡터를 $q_a, q_b$라 할 때,\\[q_a = T_{ab} q_b\\](이때 차원을 맞추기 위하여 $q_a = \\begin{bmatrix} q_x &amp;amp; q_y &amp;amp; q_z &amp;amp; 1\\end{bmatrix}^T$로 정의한다.)3. Screw motion and SE(3)Chasles’ - Mozzi theorem에 의하면 모든 강체 운동은 그에 대응하는 나선 운동(Screw motion) 이 존재한다. 따라서 임의의 $T \\in SE(3)$은 그에 대응하는 나선 운동을 찾을 수 있다.$T_{01} = \\begin{bmatrix} R &amp;amp; p \\\\ 0 &amp;amp; 1\\end{bmatrix}, \\;\\; T_{02} = \\begin{bmatrix} R’ &amp;amp; p’ \\\\ 0 &amp;amp; 1\\end{bmatrix}$라 두자. $T_{02}$는 그림에서 파란색으로 표시한 3개의 화살표를 통해서도 계산할 수 있다. 첫번째 화살표 : 임의의 회전축 위의 한 점을 향하는 벡터 $q$ 두번째 화살표 : screw axis $\\hat{\\omega}$를 기준으로 $(p-q)$를 $\\theta$만큼 회전시켰으므로 $(p-q)e^{[\\hat{\\omega}]\\theta}$ 세번째 화살표 : screw axis 방향으로 $d$만큼 이동하므로 $d\\hat{\\omega} = h \\theta \\hat{\\omega}$(이때 $h$는 pitch of screw, i.e. $h = \\frac{d}{\\theta}$)따라서\\[p&#39; = q + (p-q)e^{[\\hat{\\omega}]\\theta} + h \\theta \\hat{\\omega}\\]로 나타낼 수 있다. 회전행렬의 경우 screw axis를 기준으로 회전시킨 경우만 고려하면 되므로,\\[R&#39; = e^{[\\hat{\\omega}]\\theta}\\]이다. 이 둘을 종합하면\\[\\begin{bmatrix} R&#39; &amp;amp; p&#39; \\\\ 0 &amp;amp; 1\\end{bmatrix} = \\begin{bmatrix} e^{[\\hat{\\omega}]\\theta} &amp;amp; (I - e^{[\\hat{\\omega}]\\theta})q + h \\theta \\hat{\\omega} \\\\ 0 &amp;amp; 1\\end{bmatrix} \\begin{bmatrix} R &amp;amp; p \\\\ 0 &amp;amp; 1\\end{bmatrix}\\]이 성립한다.5. Exponential Coordinate of SE(3)$SO(3)$ group에서 $R = e^{[\\hat{\\omega}]\\theta}$와 같이 exponential form으로 표현한 것 처럼 $T \\in SE(3)$ 역시 $e^{[\\mathcal{S}]\\theta}$와 같은 exponential form으로 나타낼 수 있다. 이때\\[\\mathcal{S} = \\begin{bmatrix} \\hat{\\omega} \\\\ v\\end{bmatrix} \\in \\mathbb{R}^6, \\;\\; [\\mathcal{S}] = \\begin{bmatrix} [\\hat{\\omega}] &amp;amp; v \\\\ 0 &amp;amp; 0\\end{bmatrix} \\in se(3)\\]로 표현한다. 주의할 점은 $se(3)$에 속하는 $4 \\times 4$ matrix는 4행 4열 성분이 0이라는 점이다. $SE(3)$에 속하는 matrix는 4행 4열 성분이 1이었다는 점을 상기하자.위 식에서 $\\hat{\\omega}$는 screw axis 단위 방향 벡터이다. Tayler expansion을 이용하여 식을 변형하면 다음과 같다.\\[\\begin{align*}\\begin{bmatrix} e^{[\\hat{\\omega}]\\theta} &amp;amp; (I - e^{[\\hat{\\omega}]\\theta})q + h \\theta \\hat{\\omega} \\\\ 0 &amp;amp; 1\\end{bmatrix}&amp;amp;= e^{[\\mathcal{S}]\\theta}= I + [\\mathcal{S}]\\theta + \\frac{1}{2!} [\\mathcal{S}]^2 \\theta^2 + \\cdots \\\\&amp;amp;= I + \\begin{bmatrix} [\\hat{\\omega}] &amp;amp; v \\\\ 0 &amp;amp; 0\\end{bmatrix} \\theta + \\frac{1}{2!} \\begin{bmatrix} [\\hat{\\omega}] &amp;amp; v \\\\ 0 &amp;amp; 0\\end{bmatrix}^2 \\theta^2 + \\frac{1}{3!}\\begin{bmatrix} [\\hat{\\omega}] &amp;amp; v \\\\ 0 &amp;amp; 0\\end{bmatrix}^3 \\theta^3 + \\cdots \\\\&amp;amp;= I + \\begin{bmatrix} [\\hat{\\omega}] &amp;amp; v \\\\ 0 &amp;amp; 0\\end{bmatrix} \\theta + \\frac{1}{2!} \\begin{bmatrix} [\\hat{\\omega}]^2 &amp;amp; [\\hat{\\omega}]v \\\\ 0 &amp;amp; 0\\end{bmatrix} \\theta^2 + \\frac{1}{3!}\\begin{bmatrix} -[\\hat{\\omega}] &amp;amp; [\\hat{\\omega}]^2 v \\\\ 0 &amp;amp; 0\\end{bmatrix} \\theta^3 + \\cdots \\\\&amp;amp;= \\begin{bmatrix} e^{[\\hat{\\omega}]\\theta} &amp;amp; G(\\theta)v \\\\ 0 &amp;amp; 1\\end{bmatrix}\\end{align*}\\]이때 $[\\hat{\\omega}]^3 = - [\\hat{\\omega}]$라는 사실을 이용하였다. $G(\\theta)$는 4열 성분을 분석하면 다음과 같이 계산할 수 있다.\\[\\begin{align*}G(\\theta)v &amp;amp;= 0 + v\\theta + \\frac{1}{2!}[\\hat{\\omega}]v \\theta^2 + \\frac{1}{3!}[\\hat{\\omega}]^2 v \\theta^3 + \\cdots\\\\&amp;amp;=\\left( I \\theta + \\frac{1}{2!}[\\hat{\\omega}] \\theta^2 + \\frac{1}{3!}[\\hat{\\omega}]^2 \\theta^3 + \\cdots \\right)v \\\\&amp;amp;=\\left( I \\theta + [\\hat{\\omega}] \\left(\\frac{1}{2!}\\theta^2 - \\frac{1}{4!}\\theta^4 + \\cdots \\right) + [\\hat{\\omega}]^2 \\left(\\frac{1}{3!}\\theta^3 - \\frac{1}{5!}\\theta^5 + \\cdots \\right)\\right)v \\\\&amp;amp;=\\left( I \\theta + (1-\\cos{\\theta})[\\hat{\\omega}] + (\\theta - \\sin{\\theta})[\\hat{\\omega}]^2 \\right)v\\end{align*}\\]\\[\\therefore G(\\theta) = \\left( I \\theta + (1-\\cos{\\theta})[\\hat{\\omega}] + (\\theta - \\sin{\\theta})[\\hat{\\omega}]^2 \\right)\\]\\[(I - e^{[\\hat{\\omega}]\\theta})q + h \\theta \\hat{\\omega} = \\left( I \\theta + (1-\\cos{\\theta})[\\hat{\\omega}] + (\\theta - \\sin{\\theta})[\\hat{\\omega}]^2 \\right)v\\]또한 $v$는 다음과 같다. 유도하지는 않지만, 대입을 통해 확인만 해보도록 한다.\\[v = -\\hat{\\omega} \\times q + h\\hat{\\omega} = -[\\hat{\\omega}]q + h\\hat{\\omega}\\]Check\\[\\left( I \\theta + (1-\\cos{\\theta})[\\hat{\\omega}] + (\\theta - \\sin{\\theta})[\\hat{\\omega}]^2 \\right) \\left( -[\\hat{\\omega}]q + h\\hat{\\omega} \\right)\\]\\[\\begin{align*}&amp;amp;=-[\\hat{\\omega}]\\theta q + h \\theta \\hat{\\omega} - \\left(1-\\cos{\\theta}\\right)[\\hat{\\omega}]^2 q + \\left(\\theta - \\sin{\\theta}\\right)[\\hat{\\omega}]q \\\\&amp;amp;=\\left(-\\sin{\\theta}[\\hat{\\omega}] - (1-\\cos{\\theta})[\\hat{\\omega}]^2\\right)q + h \\theta \\hat{\\omega} \\\\&amp;amp;= (I - e^{[\\hat{\\omega}]\\theta})q + h \\theta \\hat{\\omega}\\end{align*}\\]식 전개 과정에서 $[\\hat{\\omega}]\\hat{\\omega} = \\hat{\\omega} \\times \\hat{\\omega} = 0$이 사용되었다. 또한 $e^{[\\hat{\\omega}]\\theta} = I + \\sin{\\theta} [\\hat{\\omega}] + (1 - \\cos{\\theta}) [\\hat{\\omega}]^2$을 적용하였다.$SO(3)$과 마찬가지로 matrix logarithm을 정의할 수 있다. 그 알고리즘은 아래와 같다. $R = I \\; : \\; \\hat{\\omega} = 0, \\; v = \\frac{p}{|p|}, \\; \\theta = |p|$ $\\textrm{Otherwise}, \\;\\; \\hat{\\omega}\\theta = \\log{R}, \\;v = G^{-1} (\\theta)p, \\;\\; \\textrm{where}$\\[G^{-1}(\\theta) = \\frac{1}{\\theta} I - \\frac{1}{2}[\\hat{\\omega}] + \\left( \\frac{1}{\\theta} - \\frac{1}{2} \\cot{\\frac{\\theta}{2}}\\right)[\\hat{\\omega}]^2\\]4. Twist or Spacial velocity$SO(3)$에서 $[\\omega_s] = \\dot{R} R^{-1}$, $[\\omega_b] = R^{-1} \\dot{R}$이 성립하였던 것처럼, $SE(3)$에서도 유사한 관계가 존재한다.\\[\\begin{align*}\\dot{T} T^{-1} &amp;amp;= \\begin{bmatrix} \\dot{R} &amp;amp; \\dot{p} \\\\ 0 &amp;amp; 0\\end{bmatrix} \\begin{bmatrix} R^T &amp;amp; -R^T p \\\\ 0 &amp;amp; 1\\end{bmatrix} \\\\&amp;amp;= \\begin{bmatrix} \\dot{R} R^T &amp;amp; -\\dot{R} R^T p + \\dot{p} \\\\ 0 &amp;amp; 0\\end{bmatrix} \\\\&amp;amp;= \\begin{bmatrix} [\\omega _ s] &amp;amp; \\dot{p} - [\\omega _ s]p \\\\ 0 &amp;amp; 0\\end{bmatrix} =: \\begin{bmatrix} [\\omega _ s] &amp;amp; v_s \\\\ 0 &amp;amp; 0\\end{bmatrix} \\in se(3)\\end{align*}\\]$\\dot{T} T^{-1}$의 계산 결과를 분석해보자. $[\\omega _ s]$는 각속도의 skew-symmetric form을 space coordinate에서 표현한 값이다. $[\\omega _ s]p = \\omega _ s \\times p$인 점에 착안하면 $\\dot{p} - [\\omega _ s]p$는 {s} frame의 원점에 해당하는 강체 상 한점이 움직이는 선속도를 space coordinate에서 나타낸 값이다. 자세한 내용은 그림을 참고하자.즉, $\\dot{T} T^{-1}=:[\\mathcal{V}_s]$는 {s} frame에서 표현한 {s} frame의 원점의 각속도와 선속도를 의미한다. Skrew motion으로 해석하였던 강체의 운동은 실재적인 물리적 의미를 갖는다. 따라서 $[\\mathcal{V}_s]$를 spatial velocity in space frame 또는 space twist라고 정의한다.\\[\\begin{align*}T^{-1} \\dot{T} &amp;amp;= \\begin{bmatrix} R^T &amp;amp; -R^T p \\\\ 0 &amp;amp; 1\\end{bmatrix} \\begin{bmatrix} \\dot{R} &amp;amp; \\dot{p} \\\\ 0 &amp;amp; 0\\end{bmatrix} \\\\&amp;amp;= \\begin{bmatrix} R^T \\dot{R} &amp;amp; R^T\\dot{p} \\\\ 0 &amp;amp; 0\\end{bmatrix} =: \\begin{bmatrix} [\\omega _ b] &amp;amp; v_b \\\\ 0 &amp;amp; 0\\end{bmatrix} \\in se(3)\\end{align*}\\]마찬가지로 $T^{-1} \\dot{T}$를 계산해보면 $[\\omega _ b]$는 {b} frame에서 나타낸 강체의 각속도를 의미하고, $v_b = R^T \\dot{p}$는 {b} frame에서 나타낸 {b} frame의 원점의 선속도를 의미한다. 따라서 $[\\mathcal{V}_b] := T^{-1} \\dot{T}$를 spatial velocity in body frame 또는 body twist라고 정의한다.$[\\mathcal{V}_s]=\\dot{T} T^{-1}, \\;\\; [\\mathcal{V}_b] = T^{-1} \\dot{T}$에서\\[\\begin{align*}[\\mathcal{V}_s] &amp;amp;= T [\\mathcal{V}_b] T^{-1} \\\\&amp;amp;= \\begin{bmatrix} R &amp;amp; p \\\\ 0 &amp;amp; 1\\end{bmatrix} \\begin{bmatrix} [\\omega _ b] &amp;amp; v_b \\\\ 0 &amp;amp; 0\\end{bmatrix} \\begin{bmatrix} R^T &amp;amp; -R^T p \\\\ 0 &amp;amp; 1\\end{bmatrix} \\\\&amp;amp;= \\begin{bmatrix} R [\\omega _ b] R^T &amp;amp; - R [\\omega _ b] R^T p + R v_b \\\\ 0 &amp;amp; 0\\end{bmatrix} \\\\&amp;amp;= \\begin{bmatrix} [R \\omega _ b] &amp;amp; [p]R \\omega _ b + R v_b \\\\ 0 &amp;amp; 0\\end{bmatrix}\\end{align*}\\]\\[\\therefore \\omega _ s = R \\omega _b , \\;\\;\\; v_s = [p]R \\omega _ b + R v_b\\]\\[\\mathcal{V}_s = \\begin{bmatrix} \\omega _ s \\\\ v_s \\end{bmatrix} = \\begin{bmatrix} R &amp;amp; 0 \\\\ [p]R &amp;amp; R\\end{bmatrix} \\begin{bmatrix} \\omega _ b \\\\ v_b \\end{bmatrix} = \\begin{bmatrix} R &amp;amp; 0 \\\\ [p]R &amp;amp; R\\end{bmatrix} \\mathcal{V}_b\\]와 같은 변환 관계가 성립한다. 이처럼 두 spatial velocitys(twists)는 $6 \\times 6$ matrix인 adjoint representation을 통해 변환된다.\\[[\\textrm{Ad}_T] := \\begin{bmatrix} R &amp;amp; 0 \\\\ [p]R &amp;amp; R\\end{bmatrix}\\]\\[\\mathcal{V}&#39; = [\\textrm{Ad}_T] \\mathcal{V} \\;\\; \\textrm{or} \\;\\; [\\mathcal{V}&#39;] = \\textrm{Ad}_T(\\mathcal{V})\\]" }, { "title": "[Lie group] SO(3) group 정리", "url": "/posts/SO3_group/", "categories": "SLAM", "tags": "lie group, SO(3), rotation, angular velocity, skew-symmetric, Rodrigues' rotation formula, exponential, logarithm", "date": "2022-09-15 00:00:00 +0900", "snippet": "LIO-SAM 논문 리딩을 위해 IMU preintegration에 대해 공부하다 보니 Lie group에 대한 깊은 이해가 필요하다는 것을 느꼈다. 3학년 1학기 때 “로봇 공학 입문” 수업을 들으며 Lie group에 대해 공부했었는데, 향후 논문 리딩에 많이 참고할 것 같아 따로 정리해 두려고 한다. 아래 등장하는 그림들은 수업 시간에 필기한 자료를 가져왔음을 미리 밝혀둔다. reference textbookLynch, Kevin M., and Frank C. Park. Modern robotics. Cambridge University Press, 2017.1. Special Orthogonal Group, SO(3)그림은 space frame(s)에서 바라본 body frame(b)를 나타내었다. 이때 두 좌표계의 기저 벡터 사이 관계는 다음과 같은 $3 \\times 3$ matrix로 기술할 수 있을 것이다.\\[\\begin{bmatrix} \\hat{x}_b \\\\ \\hat{y}_b \\\\ \\hat{z}_b \\end{bmatrix}=\\begin{bmatrix} r_{11} &amp;amp; r_{12} &amp;amp; r_{13} \\\\r_{21} &amp;amp; r_{22} &amp;amp; r_{23} \\\\r_{31} &amp;amp; r_{32} &amp;amp; r_{33} \\end{bmatrix}\\begin{bmatrix} \\hat{x}_s \\\\ \\hat{y}_s \\\\ \\hat{z}_s \\end{bmatrix}\\]이때 $R=[r_{ij}]$가 만족해야 할 조건은 다음과 같다. Unit vector : $| \\hat{x} _ {b} | = | \\hat{y} _ {b} | = | \\hat{z} _ {b} | = 1$ i.e. $\\sum_{i=1}^3 r_{ij}^2 = 1 (j=1, 2, 3)$ Orthogonality : $\\hat{x} _ {b} \\cdot \\hat{y} _ {b} = \\hat{y} _ {b} \\cdot \\hat{z} _ {b} = \\hat{z} _ {b} \\cdot \\hat{x} _ {b} = 0$ Right-hand coordinate : $\\det{R} = 1$이러한 3개의 조건을 만족하는 $3 \\times 3$ matrix은 3차원 상의 회전을 기술한다. 이들의 집합을 SO(3)이라고 한다. 한편 위 상황과 같이 {s}에서 바라본 {b}의 rotation을 $R_{sb}$로 나타낸다.2. SO(3) 성질SO(3) 행렬이 만족하는 성질을 정리하면 다음과 같다. $RR^T=R^T R = I$ $R^T = R^{-1}$ $R_1, R_2 \\in SO(3) \\Rightarrow R_1 R_2 \\in SO(3)$ 특히, $R_{ab}, R_{bc} \\in SO(3) \\Rightarrow R_{ab} R_{bc} = R_{ac} \\in SO(3)$ 동일한 벡터 $\\vec{v}$를 서로 다른 두 좌표계 {a}, {b}에서 기술할 때, $\\vec{v_a} = R_{ab} \\vec{v_b}$3. Angular velocity and skew-symmetric matrix그림과 같이 각속도 벡터 $\\omega = \\dot{\\theta} \\hat{\\omega}$에 의해 좌표계가 회전하는 상황을 고려하자. 이때 단위벡터 $\\hat{x}$가 시간에 따라 변화하는 방향 변화율은\\[\\dot{\\hat{x}} = \\omega \\times \\hat{x}\\]로 기술할 수 있다. 이는 $\\hat{y}, \\hat{z}$에 대해서도 마찬가지이다.이 식에 $\\begin{bmatrix} \\hat{x}_b &amp;amp; \\hat{y}_b &amp;amp; \\hat{z}_b \\end{bmatrix}^T=R\\begin{bmatrix} \\hat{x}_s &amp;amp; \\hat{y}_s &amp;amp; \\hat{z}_s \\end{bmatrix}^T$을 대입했다고 생각하면 $R$의 각 열벡터 $r_i (i=1, 2, 3)$는 다음이 성립한다.\\[\\dot{r_i} = \\omega_s \\times r_i\\]열벡터를 종합하면 시간에 따른 $SO(3)$ matrix $R$의 변화율 $\\dot{R}(t)$를 다음과 같이 기술할 수 있다.\\[\\dot{R} = \\begin{bmatrix} \\omega_s \\times r_1&amp;amp; \\omega_s \\times r_2 &amp;amp; \\omega_s \\times r_3 \\end{bmatrix} = [\\omega_s]R\\]이때 $[\\omega_s]$ skew-symmetric(i.e. $[\\omega_s]^T = -[\\omega_s]$)을 만족하는 행렬이다. 이러한 skew-symmetric matrix를 모아둔 집합을 $so(3)$이라고 한다.\\[[\\omega] = \\begin{bmatrix} 0 &amp;amp; -\\omega_3 &amp;amp; -\\omega_2 \\\\\\omega_3 &amp;amp; 0 &amp;amp; -\\omega_1 \\\\-\\omega_2 &amp;amp; \\omega_1 &amp;amp; 0 \\end{bmatrix}\\]몇가지 성질을 아래에 정리해 두었다. $R \\in SO(3), \\;\\; R [\\omega] R^T = [R \\omega]$ $\\dot{R} = [\\omega_s]R$으로부터, $[\\omega_s] = \\dot{R} R^{-1}= \\dot{R} R^{T}$ $\\omega_b$를 body frame에서 기술한 각속도 벡터라고 할 때,\\[[\\omega_b] = [R^T \\omega_s] = R^T[\\omega_s]R = R^T(\\dot{R} R^{-1})R = R^{-1} \\dot{R}\\]4. Exponential Coordinate of SO(3)벡터 $p$가 크기가 1인 unit angular velocity 벡터 $\\hat{\\omega}$를 따라 회전할 때 $p$의 속도 $\\dot{p}$는\\[\\dot{p} = \\hat{\\omega} \\times p = [\\hat{\\omega}]p\\]이다. 이는 matrix differential equation form이고, 초기조건 $p(0)$이 주어졌을 때 해는\\[p(t) = e^{[\\hat{\\omega}]t}p(0)\\]\\[p(\\theta) = e^{[\\hat{\\omega}]\\theta}p(0)\\]이다. 특히 exponential part를 tayler expansion하면\\[e^{[\\hat{\\omega}]\\theta} = I + [\\hat{\\omega}]\\theta + [\\hat{\\omega}]^2 \\frac{\\theta ^ 2}{2} + \\cdots\\]이 성립한다. [Remark] Cayley-Hamilton theorem행렬 A의 특성다항식\\[P(s) = \\det{(sI-A)}=s^n + a_{n-1}s^{n-1} + \\cdots + a_0\\] 에 대하여 다음 식이 성립한다.\\[P(A) = A^n + a_{n-1}A^{n-1} + \\cdots + a_0 = O\\]$|\\hat{\\omega}|=1$에 유의하여 $[\\hat{\\omega}]$의 특성다항식을 계산하면\\[P(s) = s^3 + s\\]이므로\\[P([\\hat{\\omega}]) = [\\hat{\\omega}]^3 + [\\hat{\\omega}] = O\\]가 성립한다. 이를 이용하면 exponential part는\\[\\begin{align*}e^{[\\hat{\\omega}]\\theta} &amp;amp;= I + [\\hat{\\omega}]\\theta + [\\hat{\\omega}]^2 \\frac{\\theta ^ 2}{2!} + [\\hat{\\omega}]^3 \\frac{\\theta ^ 3}{3!} + \\cdots \\\\&amp;amp;= I + [\\hat{\\omega}]\\theta + [\\hat{\\omega}]^2 \\frac{\\theta ^ 2}{2!} - [\\hat{\\omega}] \\frac{\\theta ^ 3}{3!} + \\cdots \\\\&amp;amp;= I + \\left( \\theta - \\frac{\\theta ^ 3}{3!} + \\cdots \\right) [\\hat{\\omega}] + \\left( \\frac{\\theta ^ 2}{2!} - \\frac{\\theta ^ 4}{4!} + \\cdots \\right) [\\hat{\\omega}]^2 \\\\&amp;amp;= I + \\sin{\\theta} [\\hat{\\omega}] + (1 - \\cos{\\theta}) [\\hat{\\omega}]^2\\end{align*}\\]와 같이 정리된다. 따라서 3차원 상에서 회전축 $\\hat{\\omega}$와 각도 $\\theta$를 알 때 $SO(3)$ 행렬 $R$을\\[R = e^{[\\hat{\\omega}]\\theta} = I + \\sin{\\theta} [\\hat{\\omega}] + (1 - \\cos{\\theta}) [\\hat{\\omega}]^2\\]로 나타낼 수 있는데 이를 Rodrigues’ rotation formula라고 한다.$[\\hat{\\omega}]\\theta \\in so(3)$에서 $R \\in SO(3)$으로 가는 변환을 matrix exponential로 나타낼 수 있으므로 그 역변환인 matrix logarithm을 생각할 수 있다. 이는 다음과 같은 알고리즘으로 계산 가능하다. $R = I \\; : \\; \\theta = 0, \\; \\; \\hat{\\omega} : \\textrm{undefined}$ $\\textrm{tr}(R) = -1 \\; : \\; \\theta = \\pi, \\;\\; \\textrm{solve} \\;\\; R = I + 2 [\\hat{\\omega}]^2$ $\\textrm{Otherwise}, \\;\\; \\cos{\\theta} = \\frac{1}{2}(\\textrm{tr}(R)-1), \\;\\; [\\hat{\\omega}] = \\frac{1}{2\\sin{\\theta}}(R - R^T)$" }, { "title": "[LeGO-LOAM] LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain 논문 리뷰", "url": "/posts/LeGO_LOAM_review/", "categories": "SLAM", "tags": "lego-loam, loam, lidar, odometry, mapping, ground, segmentation, Levenberg, Marquardt, LM, loop closure", "date": "2022-09-13 00:00:00 +0900", "snippet": "LeGO-LOAM은 ground를 이용하여 LOAM 알고리즘을 경량화하고 성능과 속도를 향상시켰다. LOAM과 비교하여 개선된 점을 위주로 논문을 정리해보았다. Original paper LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain 다음 자료를 참고하였습니다. LOAM, Lego-LOAM SLAM 온라인 스터디 SLAM DUNK 2020 | 정진용님 발표 LeGO-LOAM Line by Line 1. IntroductionLOAM은 smoothness를 기준으로 edge, planar feature를 분류한 다음, 점들 사이의 scan-matching을 이용하여 두 scan 간의 대응 관계(correspondence)를 탐색한다. 실시간으로 알고리즘이 구동될 수 있게끔 추정 문제를 2개의 개별 알고리즘으로 나누어 해결한다. 하나(LiDAR Odometry)는 높은 주파수와 낮은 정확도로 센서의 속도를 추정하고, 다른 하나(LiDAR Mapping)는 낮은 주파수와 높은 정확도로 모션 추정을 수행한다. 이 두 추정값은 Transform Integration 단계에서 융합되어 높은 정확도와 낮은 drift를 보이게 된다.본 논문은 UGV(Unmanned Ground Vehicle)에서 구동할 수 있는 LOAM을 제안하는데 목적을 두었다. LOAM은 모든 points에서 smoothness를 계산해야하므로 계산 리소스가 제한된 상황에 부적절하다. LiDAR가 지면에 가깝게 장착되는 UGV 특성 상 잔디 등의 요인으로 신뢰할 수 없는 edge feature가 추출될 수 있다. 이러한 요인으로 인해 부정확한 registration과 큰 drift가 발생할 우려가 있다.이를 해결하고자 논문에서는 몇 가지 contribution points를 제안한다. Pointcloud segmentation을 거친 후 ground와 신뢰할 수 없는 points를 제거 Pose estimation 단계에서 optimization을 2 단계로 수행하여 계산 복잡도를 감소 Drift를 보정하기 위해 loop closure를 수행2. Notation and System Block Diagram출처: 원 논문 Right subcription $t$ : time of Sweeps $P_t$ : time $t$에 얻어진 pointcloud, $P_t = \\{p_1, p_2, \\cdots, p_n\\}$ $r_i$ : i번째 점의 depth $\\mathbb{F}_e^t, \\mathbb{F}_p^t$ : time $t$에서 얻은 모든 sub-images에서의 edge, planar features 집합3. Segmentation3.1 Ground Removal본 논문은 LiDAR가 지면과 평행하게 부착되어 있다고 가정한다. VLP-16을 예시로 들자면 한 각도에서 vectical한 방향으로 16개의 laser가 발사되어 거리가 측정된다. 이때 아래(ground) 방향으로 발사되는 8개 channel에 대해 Ground Removal 단계가 수행된다.출처: LeGO-LOAM Line by Line - 2. ImageProjection (2)Vectical하게 인접한 두 픽셀에 대해 LiDAR 좌표계에서의 $(x, y, z)$를 계산한다. 다음으로 그림과 같이 지면과 이루는 각도를\\[\\theta = \\arctan \\frac{\\Delta z}{\\sqrt{\\Delta x^2 + \\Delta y^2}}\\]로 계산한다. 그 각도가 10도보다 낮은 점을 ground로 판별한다. [Note] Ground segmentation 성능논문에서 제시하고 있는 방법은 효율적이지 않다고 하는데 그 원인은 다음과 같다. 한 픽셀 내에 여러 points가 찍히는 경우 실제 ground points가 ground가 판별되지 않음 기울기가 불규칙한 지역에서 성능이 상당히 저하됨 따라서 다른 용도로 ground segmentation을 적용할 필요가 있을 경우, 다른 알고리즘을 사용하는 것이 좋을 듯 하다.3.2 Cloud Segmentation지면을 분리한 pointclound에 image-based segmentation을 적용한다. 본 논문은 “Fast Range Image-Based Segmentation of Sparse 3D Laser Scans for Online Operation” 을 이용하여 segmentation을 수행하였다고 밝혔다. Reference paper Fast Range Image-Based Segmentation of Sparse 3D Laser Scans for Online Operation 출처: “Fast Range Image-Based Segmentation of Sparse 3D Laser Scans for Online Operation”$\\beta$를 laser 빔(line $OA$)과 인접한 두 픽셀을 이은 선(line $AB$)과의 각도로 정의한다. 이는 기하학적으로\\[\\beta = \\arctan \\frac{d_2 \\sin{\\alpha}}{d_1 - d_2 \\cos{\\alpha}}\\]로 계산 가능하다. 이때 $d_1, d_2$는 각각 점 A, B의 depth를 의미한다. $\\beta$가 특정 threshold보다 작은 경우, 즉 오른쪽 그림에서 빨간색으로 표시한 경우, 경계면이 급격하게 변화하므로 두 점은 다른 물체에 포함되어 있다고 간주할 수 있다. 이러한 heuristic한 방법을 이용하여 BFS 기반으로 image segmentation &amp;amp; labeling을 수행할 수 있다.수가 충분히 많지 않는(30개 이하) cluster는 robustness를 위해 유효한 점에서 제외한다.4. LiDAR Odometry출처: 원 논문Feature Extraction 단계는 LOAM과 동일하여 생략한다. LiDAR Odometry 단계 역시 유사하지만, 몇 가지 차이점이 존재한다.4.1 Label matchingSegmented cluster에서는 edge feature($\\mathbb{F}_e$)의 correspondence만 탐색하고, ground cluster에서는 planar feature($\\mathbb{F}_p$)의 correspondence만 탐색한다. 즉, feature extraction 단계에서 planar feature는 ground에서만, edge feature는 segmented cluster에서만 뽑으면 되므로, 계산 시간을 단축할 수 있다.4.2 Two-step L-M optimizationPlanar feature를 이용하여 $[t_z, {\\theta} _ {roll}, {\\theta} _ {pitch}]$를 optimize한 후 edge feature를 이용하여 $[t_x, t_y, {\\theta} _ {yaw}]$를 optimize한다. 각각 과정에서 optimize하지 않는 parameter는 상수로 취급한다. 이를 통해 계산 시간을 35% 감소시키면서 유사한 정확도를 얻을 수 있다고 한다.5. LiDAR Mapping기본 아이디어는 LOAM과 동일하지만, pointcloud map을 저장할 때 feature set $\\{ \\mathbb{F} _ {e}, \\mathbb{F} _ {p} \\}$을 저장한다는 차이가 있다. $M_t = \\{\\{\\mathbb{F} _ {e}^1, \\mathbb{F} _ {p}^1\\}, \\cdots, \\{\\mathbb{F} _ {e}^t, \\mathbb{F} _ {p}^t\\}\\}$라 정의할 때, $M_{t-1}$에서 $\\bar{Q}_{t-1}$을 얻는 방법을 논문에서는 2 가지 제시하였다. 현재 pose를 기준으로 저장된 feature 중 주변 100m 이내에 있는 모든 pose들의 feature들을 불러온 다음, 모든 feature들을 각 pose로 transform하여 얻는다. LeGO-LOAM을 pose-graph SLAM으로 통합하여 생각한다. 즉, 로봇의 pose는 graph node로, feature set은 node의 measurement로 모델링한다.그 후 LOAM과 마찬가지로 L-M optimization을 수행하여 transform matrix를 얻는다.추가로 논문에서는 loop closure를 수행하여 추가적인 contraint를 얻으면 drift를 줄일 수 있을 것이라 한다." }, { "title": "[LOAM] LOAM: Lidar Odometry and Mapping in Real-time 논문 리뷰", "url": "/posts/LOAM_review/", "categories": "SLAM", "tags": "loam, lidar, odometry, mapping, Levenberg, Marquardt, LM, edge, planar", "date": "2022-09-10 00:00:00 +0900", "snippet": "3학년 여름방학동안 김아영 교수님 연구실에서 UROP(Undergraduate Research Opportunity Program) 연구를 수행하였다. 연구 과제는 “4족 보행 로봇 Spot을 위한 HW/SW 시스템 구축”으로, Spot에 탑재할 센서 시스템(LiDAR, IMU, GNSS, Camera) 하드웨어를 설계하고 SLAM 알고리즘을 돌려보는 활동으로 구성되었다. 본 포스팅부터는 여름방학동안 학습한 SLAM 논문을 리뷰하고 Spot으로 실제 측정한 데이터를 바탕으로 SLAM을 돌려보는 과정을 정리해보고자 한다. Original paper LOAM: Lidar Odometry and Mapping in Real-time 다음 자료를 참고하였습니다. LOAM, Lego-LOAM [SLAM] Lidar Odometry And Mapping (LOAM) 논문 리뷰 SLAM 온라인 스터디 SLAM DUNK 2020 | 정진용님 발표 LeGO-LOAM Line by Line 1. IntroductionLOAM은 논문 제목에서 알 수 있듯이 LiDAR 센서만을 이용하여 odometry와 mapping을 실시간으로 수행하는 SLAM이다. 기존 SLAM 경우 off-line, 즉 데이터 수집 후 프로그램을 돌리는 방식으로 3D mapping을 수행하였고, drift를 보정하기 위해 loop closure를 사용하였다. 그러나 LOAM은 계산 복잡도를 줄여 실시간 SLAM이 가능하도록 하였는데, 이는 전체 알고리즘을 odometry를 계산하는 블록과 mapping 블록으로 나누어 병렬적으로 처리하였기 때문에 가능하였다. [Note] Terminology SLAM : Simultaneous Localization and Mapping의 약자이다. 로봇이 주변 환경을 센서로 감지하여 주변 map을 제작하고, map 내에 현재 자신의 위치를 추정하는 작업을 동시에 진행하는 기술이다. LiDAR : 레이저 센서가 360도 회전(sweep)하면서 주위 환경을 스캔하는 장치이다. 다른 센서와 비교하여 상당한 거리 정밀도를 보이고, 주변 조명과 재질에 둔감하다는 장점이 있다. 센서가 움직이면서 mapping하는 경우 LiDAR의 위치를 계속 추정하는 작업이 필요하다. Odometry : 로봇이 주행한 궤적을 의미한다. IMU나 wheel encoder를 이용하여 odometry를 추정할 경우 센서 데이터를 계속 적분하게 되므로 시간에 따라 오차가 누적된다. 따라서 궤적이 전체적으로 틀어지는 drift 문제가 발생할 수 있다. 이를 해결하고자 로봇이 이전에 방문했던 위치에 되돌아왔는지 판단하는 loop closure detection을 도입하기도 한다. 본 논문은 loop closure를 포함하지 않고 SLAM 문제를 해결하였다.2. Notation Right subcription $k$ : # of Sweeps $\\mathcal{P}_k$ : k번째 sweep으로 얻어진 pointclound 데이터 $\\{ L \\}$ : LiDAR coordinate system $\\{ W \\}$ : World coordinate system $X_{(k,i)}^{L}$ : LiDAR coordinate system에서 k번째 sweep으로 얻은 i번째 점 $X_{(k,i)}^{W}$ : World coordinate system에서 k번째 sweep으로 얻은 i번째 점3. System Block Diagram출처: 원 논문 LiDAR Odometry : 2개의 연속적인 sweeps을 바탕으로 상대 운동을 계산하는 노드, 10Hz로 수행 LiDAR Mapping : Odometry를 바탕으로 pointcloud를 world coordinate system으로 매칭하는 노드, 1Hz로 수행 Transform Integration : Odometry와 mapping을 바탕으로 현재 로봇의 transform을 계산하는 노드4. LiDAR Odometry크게 Feature Point Estimation Finding Feature Point Correspondence Motion Estimation3 단계로 구성된다.4.1 Feature Point Estimation$i \\in \\mathcal{P}_k$인 한 점 $i$에 대해 $i$ 점과 horizontal하게 인접해있는 점들의 집합을 $\\mathcal{S}$라고 정의한다. 구현된 코드 상에서는 양쪽 5개씩 총 10개의 점을 사용하였다. 이때 local surface의 curvature $c$를 다음과 같이 나타낼 수 있다.\\[c = \\frac{1}{|\\mathcal{S}| \\cdot \\| X_{(k,i)}^{L} \\| } \\| \\sum_{j \\in \\mathcal{S}, j \\neq i} ( X_{(k,i)}^{L} - X_{(k,j)}^{L} ) \\|\\]출처: LeGO-LOAM Line by Line - 3. FeatureAssociation (1)식의 의미를 그림으로 해석해보자. Local surface가 planar한 경우 점들의 depth가 등차수열을 이루므로 $\\sum_{j \\in \\mathcal{S}, j \\neq i} ( X_{(k,i)}^{L} - X_{(k,j)}^{L} )$ 값이 양쪽에서 서로 상쇄되어 0에 가까운 값이 나올 것이다. 반면 local surface가 edge와 같이 뾰족할 경우 $\\sum_{j \\in \\mathcal{S}, j \\neq i} ( X_{(k,i)}^{L} - X_{(k,j)}^{L} )$ 값이 크게 나올 것이다. 즉, $c$ 값이 작은 경우 planar feature($\\mathcal{H}_k$)에 해당하고, 큰 경우 edge feature($\\mathcal{E}_k$)에 해당한다.논문에서는 1 sweep으로 얻은 데이터를 4개의 sub-region으로 나눈 후 각 sub-region에서 최대 2개의 edge points와 4개의 planar points를 사용하였다고 밝혔다. Edge points와 planar points는 $c$ 값이 특정 threshold보다 크거나 작을 경우에 해당한다.출처: 원 논문다음 상황과 같이 (a) local surface가 laser 방향과 거의 평행한 경우는 $c$를 계산할 때 정확도가 떨어진다. (b) 차폐로 인해 edge points로 오인된 경우 보는 각도에 따라 다른 feature로 인식될 수 있다. 따라서 두 경우를 outlier로 간주하여 제외한다. [Note] 구현 상 오류(?)출처: LeGO-LOAM Line by Line - 3. FeatureAssociation (1) 실제로 구현 코드에서는 valid pixel만을 사용하기 때문에 $X_{(k,j)}^{L} (j \\in \\mathcal{S})$가 일정한 간격의 horizontal한 점들로 구성되지 않을 수 있다는 오류가 있다고 한다. 그림과 같이 $X_{(k,i+5)}^{L}$가 다음 줄로 넘어가는 경우 엉뚱한 $c$ 값이 도출된다.4.2 Finding Feature Point CorrespondenceLiDAR odometry의 목적은\\[T_{k+1}^L = [ t_x, t_y, t_z, {\\theta} _{x}, {\\theta} _{y}, {\\theta} _{z} ]\\]즉, $t_{k+1}$ 시점에서 로봇의 pose를 계산하는 것이다. 이때 $t$는 평행운동 성분, ${\\theta}$는 회전운동 성분이다. 우선 sweep이 시작하기 전 $T_{k+1}^L$ 값을 초기화해준다. 논문에서는 $T_{k+1}^L = 0$으로 초기화하였으나, IMU가 있을 경우 IMU에서 측정한 속도 값으로 초기화할 수 있다.로봇이 움직이면서 laser scan이 진행되므로 한 sweep에서 측정한 pointcloud이더라도 좌표계가 바뀔 수 있다. 따라서 점들을 기준 시점에서 측정했을 때의 좌표로 transform하는 작업이 필요한데, 이를 de-skewing이라고 한다.출처: 원 논문$t \\in [t_{k+1}, t_{k+2}]$일 때, 즉 k+1번째 sweep을 수행중일 때를 보자. $P_{k}$는 완전히 갖고 있고, $P_{k+1}$은 그림과 같이 누적되고 있는 상태이다. 우리의 목표는 $t_{k+1}$ 시점에서 $P_{k}$와 $P_{k+1}$의 대응점을 탐색하는 것이다. 이를 위해서는 앞서 초기화한 $T_{k+1}^L$을 이용할 수 있다. 예를 들어 $P_{k} \\rightarrow \\bar{P_{k}}$로 transform할 때에는\\[\\bar{ X}_{(k,i)}^L = R X_{(k,i)}^L + T_{(k+1,i)}^L [1:3] \\Delta t\\]변환 공식을 이용한다. 이때 $\\Delta t$는 특정 좌표의 점을 scan했을 때 시점과 기준 시점($t_{k+1}$)과의 차이고,\\[R = e^{\\hat{\\omega} \\theta}\\]\\[\\hat{\\omega}= \\textrm{Skew matrix of } \\:\\: T_{(k+1,i)}^L [4:6] \\Delta t\\]\\[\\theta = \\textrm{Rotation angle}\\]로 정의된다. 마찬가지로 $P_{k+1}$에서 $t$까지 측정한 점들을 transform하여 $\\tilde{P}_{k+1}$을 얻는다.다음으로 edge feature, planar feature 각각에서 $\\bar{P} _ {k}$, $\\tilde{P} _ {k+1}$ 사이의 대응점을 탐색한다.4.2.1 Edge feature기본적인 아이디어는 외적을 이용하여 점과 직선 사이를 계산하는 것이다. $i \\in \\tilde{\\mathcal{E}}_{k+1}$인 edge point를 선택한다 $i$와 가깝고 연속적으로 위치한 $j, l \\in \\bar{P}_{k}$를 구한다. Curvature $c$를 계산하여 $(j, l)$이 edge feature임이 확인된 경우\\[d_{\\mathcal{E}}=\\frac{|(\\tilde{X}_{(k+1,i)}^L - \\bar{X}_{(k,j)}^L)\\times(\\tilde{X}_{(k+1,i)}^L - \\bar{X}_{(k,l)}^L)|}{|\\bar{X}_{(k,j)}^L - \\bar{X}_{(k,l)}^L|}\\]을 계산하여 edge feature에서의 거리 함수를 계산할 수 있다.4.2.2 Planar feature기본적인 아이디어는 외적을 이용하여 점과 평면 사이를 계산하는 것이다. $i \\in \\tilde{\\mathcal{H}}_{k+1}$인 edge point를 선택한다 $i$와 가깝고 연속적으로 위치한 $j, l, m \\in \\bar{P}_{k}$를 구한다. Curvature $c$를 계산하여 $(j, l, m)$이 planar feature임이 확인된 경우\\[d_{\\mathcal{H}}=\\frac{ \\begin{vmatrix}(\\tilde{X}_{(k+1,i)}^L - \\bar{X}_{(k,j)}^L) \\\\(\\bar{X}_{(k,j)}^L - \\bar{X}_{(k,l)}^L)\\times(\\bar{X}_{(k,j)}^L - \\bar{X}_{(k,m)}^L)\\end{vmatrix}}{|(\\bar{X}_{(k,j)}^L - \\bar{X}_{(k,l)}^L)\\times(\\bar{X}_{(k,j)}^L - \\bar{X}_{(k,m)}^L)|}\\]을 계산하여 planar feature에서의 거리 함수를 계산할 수 있다.4.3 Motion EstimationEdge, planar feature에서 도출한 거리 함수를 최소화하는 방향으로 $T_{k+1}^L$를 optimize해야 한다. 즉,\\[\\textrm{Goal} : \\underset{T_{k+1}^L}{\\operatorname{argmin}} {\\| \\textbf{d}\\|}\\;\\; \\textrm{where} \\;\\; \\textbf{d} = \\begin{bmatrix} d_{\\mathcal{E}} \\\\d_{\\mathcal{H}}\\end{bmatrix}= \\begin{bmatrix} f_{\\mathcal{E}}(X_{(k+1,i)}^L, T_{k+1}^L) \\\\f_{\\mathcal{H}}(X_{(k+1,i)}^L, T_{k+1}^L)\\end{bmatrix}\\]로 정리할 수 있다. 본 논문에서는 Levenberg-Marquardt 알고리즘을 사용하여 optimization을 수행한다고 밝혔다.\\[T_{k+1}^L \\leftarrow T_{k+1}^L - (J^{T}J + \\lambda \\cdot \\textrm{diag}(J^{T}J))^{-1} J^{T} \\textbf{d}(T_{k+1}^L)\\]이를 위해서는 $\\textbf{d}$의 Jacobian matrix가 필요하다. 구현 코드를 보면 직접 수식을 전개하여 계산한 것으로 보인다. [Note] Levenberg-Marquardt methodGradient descent method와 Gauss-Newton method을 결합한 방법이다. 해에서 멀리 떨어져 있을 때에는 Gradient descent method가 우세하여 빠르게 해에 수렴하고, 해 근처에 있을 때에는 Gauss-Newton method가 우세하게 작동한다. Gauss-Newton method보다 안정적으로 해를 탐색할 수 있고, 빠르게 해에 수렴한다는 장점이 있다. Gradient descent method\\[x_{k+1} = x_{k} - \\alpha J^{T} f(x_{k})\\] Gauss-Newton method\\[x_{k+1} = x_{k} - \\textrm{pinv}(J) f(x_{k}) = x_{k} - (J^{T}J)^{-1}J^{T} f(x_{k})\\] Levenberg-Marquardt method\\[x_{k+1} = x_{k} - (J^{T}J + \\alpha \\cdot \\textrm{diag}(J^{T}J))^{-1} J^{T} f(x_{k})\\]5. LiDAR MappingLiDAR Odometry와 달리 LiDAR Mapping은 sweep이 마무리될 때마다 수행된다. LiDAR Odometry 단계에서 계산한 de-skewing된 $\\bar{P} _ {k+1}$를 $\\{W\\}$에 registration한다.$k$번째 sweep이 끝난 후 world map에 registration된 pointcloud를 $Q_{k}$이라 하고, $T_{k+1}^{W}$에 의해 transform된 $\\bar{P} _ {k+1}$를 $\\bar{Q} _ {k+1}$라고 하자. 이때 우리의 목표는 LiDAR Odometry 과정과 유사하게 $T_{k+1}^{W}$를 최적화하여 적절한 $\\bar{Q}_{k+1}$를 구하는 것이다.LiDAR Odometry 단계와 마찬가지로 feature extraction, finding feature point correspondence, motion estimation 과정을 거쳐 $\\bar{Q}_{k+1}$를 계산한다. 몇 가지 차이점은 아래에 정리하였다. LiDAR Mapping은 LiDAR Odometry보다 더 작은 주기로 수행되므로(10Hz vs 1Hz) 10배 많은 point를 사용한다. 따라서 10배 많은 edge, planar feature를 사용한다. Edge, planar feature를 판별할 때 feature point 주변의 point $\\mathcal{S’}$에 대하여 eigen decomposition을 수행한다. 3개의 eigenvalue ${\\lambda} _ {1} &amp;gt; {\\lambda} _ {2} &amp;gt; {\\lambda} _ {3}$에 대해\\[{\\lambda} _ {1} \\gg {\\lambda} _ {2} &amp;gt; {\\lambda} _ {3}\\]이면 edge feature,\\[{\\lambda} _ {1} &amp;gt; {\\lambda} _ {2} \\gg {\\lambda} _ {3}\\]이면 planar feature로 분류한다.6. Transform Integration최종적으로 LiDAR의 pose는 sweep 당 1번씩(1Hz) $T_{k+1}^{W}$을 사용하여 update하고, 10Hz마다 $T_{k+1}^{L}$을 사용하여 update한다." }, { "title": "[GAN] MNIST dataset을 이용한 손글씨 생성", "url": "/posts/GAN_mnist/", "categories": "AI/ML/DL, GAN", "tags": "gan, generative model, generator, discriminator, mnist, pytorch", "date": "2022-02-23 00:00:00 +0900", "snippet": "GAN 아키텍쳐를 이용하여 MNIST 데이터셋을 학습시켜 새로운 손글씨 데이터를 생성하는 모델을 구현하였다. Dataset을 불러오고 전처리하는 코드, model를 정의하는 코드, 실제 학습을 진행하는 코드로 기능을 나누어 설계하였다. 현재 Pytorch를 공부하고 있는 입장이므로, 코드 한줄마다 자세한 설명을 담아 포스팅을 작성하였다. 관련 포스팅 [GAN] Generative Adversarial Nets 논문 리뷰 다음 자료를 참고하여 작성하였습니다. pytorch-tutorial/main.py at master · yunjey/pytorch-tutorial Fake Handwritten Digits using GAN Generation of Handwritten Numbers Using GenerativeAdversarial Networks 0. Util 함수 정의필요한 패키지 importimport matplotlib.pyplot as pltimport numpy as npimport torchimport seaborn as snsfrom PIL import Imageimport imageioimport osSeaborn 패키지는 Matplotlib을 기반으로 다양한 테마와 챠트를 제공하는 시각화 패키지이다. Imageio 패키지는 gif 파일을 만들기 위해 사용하였다.이미지 출력 함수def imshow(img, epoch, filename): img = img / 2 + 0.5 npimg = img.detach().cpu().numpy() plt.figure(figsize=(7, 7)) plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.title(&quot;Epoch {}&quot;.format(epoch+1)) plt.tight_layout() plt.show(block=False) plt.savefig(filename) plt.pause(2) plt.close()특정 epoch에서 generator에 의해 생성된 손글씨 이미지를 출력하기 위해 사용한다. detach()는 기존 tensor에서 gradient 전파가 안되는 tensor로 얕은 복사를 수행한다. 학습에 방해가 되지 않도록 2초간 이미지를 출력하고 팝업 창이 닫히도록 plt.pause(2), plt.close()를 추가하였다.모델 저장def save_checkpoint(model_list, filename): model_state = [] for m in model_list: model_state.append(m.state_dict()) state = { &#39;model_state&#39; : model_state, } torch.save(state, filename)학습이 끝까지 완료되었을 때, 혹은 학습 중간의 model_list에 있는 모든 모델의 파라미터 값을 저장한다.학습 그래프 출력 및 저장def loss_graph(gen_loss_list, dis_loss_list, filename): sns.set() plt.figure(figsize=(7, 7)) plt.subplot(2, 1, 1) plt.plot(dis_loss_list) plt.legend(&#39;dis&#39;) plt.title(&quot;Discriminator loss&quot;) plt.xlabel(&quot;epoch&quot;) plt.ylabel(&quot;loss&quot;) plt.subplot(2, 1, 2) plt.plot(gen_loss_list) plt.legend(&#39;gen&#39;) plt.title(&quot;Generator loss&quot;) plt.xlabel(&quot;epoch&quot;) plt.ylabel(&quot;loss&quot;) plt.tight_layout() plt.savefig(filename) plt.show() def pred_graph(real_pred_list, fake_pred_list, filename): sns.set() plt.figure(figsize=(7, 4)) pred_list = np.vstack((real_pred_list, fake_pred_list)).T plt.plot(pred_list) plt.legend([&#39;real&#39;, &#39;fake&#39;]) plt.title(&quot;Discriminator prediction&quot;) plt.xlabel(&quot;epoch&quot;) plt.ylabel(&quot;loss&quot;) plt.tight_layout() plt.savefig(filename) plt.show()Generator와 discriminator의 loss 값을 나타내는 그래프와, discriminator가 real data와 fake data를 예측하는 확률 값을 나타내는 그래프를 출력하고 저장한다.Gif 파일 생성def generate_gif(path, filename): path_list = [path+f&quot;/{i}&quot; for i in os.listdir(path)] path_list.sort(key=lambda x : int(x[19:-4])) image_list = [Image.open(i) for i in path_list] imageio.mimsave(filename, image_list, fps=2.0)Epoch에 따라 GAN 모델이 생성한 손글씨 데이터의 변화를 나타낸 gif 파일을 생성한다.1. Dataset 불러오기from torchvision import datasetsfrom torch.utils.data import DataLoaderimport torchvision.transforms as transformsimport utilDATA_PATH = &quot;./dataset_MNIST&quot;batch_size = 64transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])train_data = datasets.MNIST(root=DATA_PATH, train=True, download=True, transform=transform)train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, drop_last=True)torchvision.transforms는 이미지 데이터를 변형하는 메소드가 담겨있는 패키지로, transforms.Compose()를 이용해 변환을 합성하여 사용할 수 있다. 여기서는 이미지 데이터를 tensor 자료형으로 바꿔주는 ToTensor() 변환과 픽셀 데이터를 평균 $m=0.5$, 표준편차 $\\sigma=0.5$로 바꿔주는 Normalize() 변환을 사용하였다. 원래 MNIST 데이터셋의 자료 범위는 $[0, 1]$이므로 $\\frac{X-m}{\\sigma}$에 의해 $[-1, 1]$ 범위로 변환된다.datasets.MNIST()는 MNIST 데이터셋를 불러오고 transform을 적용해준다. download=True parameter는 root에 데이터셋이 없으면 자동으로 다운로드 해주는 옵션이다.torch.utils.data.DataLoader는 dataset을 batch 크기에 맞춰 데이터를 묶고, shuffle하는 기능을 제공해주는 iterator이다. drop_last=True로 설정하여 batch 크기 데이터를 묶은 후 남은 나머지는 버려준다.2. Model 정의구현 세부 사항은 다음을 참고하였다.출처: Generation of Handwritten Numbers Using GenerativeAdversarial NetworksHyper parameter에서 약간의 차이가 있다. 본 코드에서는 batch_size=64, training epoch=200를 사용하였다. Latent space의 dimension은 100으로 하였다. 또한 generator에서 Leaky_ReLU 대신 ReLU 함수를 적용하였다.마지막 레이어에 활성 함수로 tanh 함수나 sigmoid 함수를 사용하는 이유는 출력 값의 특성 때문이다. Generator는 새로운 이미지 데이터를 생성하는 모델이므로, 출력 범위를 normalize한 입력 범위와 동일한 $[-1, 1]$로 맞추기 위해 tanh를 사용한다. Discriminator는 주어진 데이터가 real data일 확률을 출력하는 모델이므로, 출력 범위를 $[0, 1]$로 맞추기 위해 sigmoid 함수를 사용한다. ReLU 함수는 상한이 존재하지 않으므로 마지막 레이어에 활성 함수로 사용할 수 없다.Generator 구현import torch.nn as nnclass Gen(nn.Module): def __init__(self, latent_dim, hidden_dim, image_size): super(Gen, self).__init__() model = [] model.extend([nn.Linear(latent_dim, hidden_dim), nn.ReLU(inplace=True)]) model.extend([nn.Linear(hidden_dim, hidden_dim*2), nn.ReLU(inplace=True)]) model.extend([nn.Linear(hidden_dim*2, hidden_dim*4), nn.ReLU(inplace=True)]) model.append(nn.Linear(hidden_dim*4, image_size)) model.append(nn.Tanh()) self.model = nn.Sequential(*model) def forward(self, x): return self.model(x)Discriminator 구현class Dis(nn.Module): def __init__(self, image_size, hidden_dim): super(Dis, self).__init__() model = [] model.extend([nn.Linear(image_size, hidden_dim*4), nn.LeakyReLU(0.2), nn.Dropout(0.3, inplace=True)]) model.extend([nn.Linear(hidden_dim*4, hidden_dim*2), nn.LeakyReLU(0.2), nn.Dropout(0.3, inplace=True)]) model.extend([nn.Linear(hidden_dim*2, hidden_dim), nn.LeakyReLU(0.2), nn.Dropout(0.3, inplace=True)]) model.append(nn.Linear(hidden_dim, 1)) model.append(nn.Sigmoid()) self.model = nn.Sequential(*model) def forward(self, x): return self.model(x)torch.nn.Module를 상속하여 구현한다. Latent dimension, hidden dimension, image size를 parameter로 받는다.레이어 구조를 출력해보면 다음과 같다.if __name__ == &quot;__main__&quot;: generator = Gen(100, 256, 28*28) discriminator = Dis(28*28, 256) print(generator) print(discriminator)Gen( (model): Sequential( (0): Linear(in_features=100, out_features=256, bias=True) (1): ReLU(inplace=True) (2): Linear(in_features=256, out_features=512, bias=True) (3): ReLU(inplace=True) (4): Linear(in_features=512, out_features=1024, bias=True) (5): ReLU(inplace=True) (6): Linear(in_features=1024, out_features=784, bias=True) (7): Tanh() ))Dis( (model): Sequential( (0): Linear(in_features=784, out_features=1024, bias=True) (1): LeakyReLU(negative_slope=0.2) (2): Dropout(p=0.3, inplace=True) (3): Linear(in_features=1024, out_features=512, bias=True) (4): LeakyReLU(negative_slope=0.2) (5): Dropout(p=0.3, inplace=True) (6): Linear(in_features=512, out_features=256, bias=True) (7): LeakyReLU(negative_slope=0.2) (8): Dropout(p=0.3, inplace=True) (9): Linear(in_features=256, out_features=1, bias=True) (10): Sigmoid() ))3. 학습필요한 패키지 import 및 메모리 초기화import torchimport torch.nn as nnfrom torchvision.utils import make_gridimport dataset, model, utilimport gcgc.collect()if torch.cuda.is_available(): torch.cuda.empty_cache()Hyper parameter 정의RESULT_DIR = &quot;./result&quot;DEVICE = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;batch_size = 64n_epoches = 200latent_dim = 100hidden_dim = 256image_size = 28*28learning_rate = 2e-4fixed_z = torch.randn(batch_size, latent_dim).to(DEVICE)ones = torch.ones(batch_size, 1).to(DEVICE)zeros = torch.zeros(batch_size, 1).to(DEVICE)gen_loss_list = []dis_loss_list = []real_pred_list = []fake_pred_list = []fixed_z는 latent space에서의 고정된 데이터에 대하여 학습이 진행됨에 따라 generator가 생성하는 이미지의 변화를 관찰하기 위해 사용하는 값이다.Model, optimizer, loss function, dataloader 선언gen = model.Gen(latent_dim, hidden_dim, image_size).to(DEVICE)dis = model.Dis(image_size, hidden_dim).to(DEVICE)op_gen = torch.optim.Adam(gen.parameters(), lr=learning_rate, betas=(0.5, 0.999))op_dis = torch.optim.Adam(dis.parameters(), lr=learning_rate, betas=(0.5, 0.999))loss_func = nn.BCELoss()train_loader = dataset.train_loadernn.BCELoss()는 $l_n = -\\left[y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n)\\right]$로 정의되는 loss function이다.학습 진행for epoch in range(n_epoches): gen.train() dis.train() for images, _ in train_loader: images = images.reshape(batch_size, -1).to(DEVICE) #generator loss gen.zero_grad() z = torch.randn(batch_size, latent_dim).to(DEVICE) fake_images = gen(z) fake_pred = dis(fake_images) gen_loss = loss_func(fake_pred, ones) gen_loss.backward() op_gen.step() #discriminator loss dis.zero_grad() real_pred = dis(images) dis_loss_real = loss_func(real_pred, ones) z = torch.randn(batch_size, latent_dim).to(DEVICE) fake_images = gen(z) fake_pred = dis(fake_images) dis_loss_fake = loss_func(fake_pred, zeros) dis_loss = dis_loss_real + dis_loss_fake dis_loss.backward() op_dis.step() with torch.autograd.no_grad(): real_pred_mean = real_pred.mean() fake_pred_mean = fake_pred.mean() real_pred_list.append(real_pred_mean.item()) fake_pred_list.append(fake_pred_mean.item()) gen_loss_list.append(gen_loss.item()) dis_loss_list.append(dis_loss.item()) print(f&#39;[Epoch {epoch+1}/{n_epoches}]&#39;) print(f&#39;[Generator loss: {gen_loss.item()} | Discriminator loss: {dis_loss.item()}]&#39;) print(f&#39;[real prediction: {real_pred_mean.item()} | fake_prediction: {fake_pred_mean.item()}]&#39;) print() if (epoch+1) % 20 == 0 or epoch == 0: gen.eval() sample_images = gen(fixed_z) sample_images = sample_images.reshape(batch_size, 1, 28, 28) util.imshow(make_grid(sample_images), epoch, RESULT_DIR+&quot;/gif/epoch {}.jpg&quot;.format(epoch+1))util.save_checkpoint([gen, dis], RESULT_DIR+&quot;/MNIST_gan.pt&quot;)util.loss_graph(gen_loss_list, dis_loss_list, RESULT_DIR+&quot;/loss_graph_gan.jpg&quot;)util.pred_graph(real_pred_list, fake_pred_list, RESULT_DIR+&quot;/prediction_gan.jpg&quot;)util.generate_gif(RESULT_DIR+&quot;/gif&quot;, RESULT_DIR+&quot;/gif_gan.gif&quot;)gen.train(), gen.eval()는 각각 모델을 train 모드와 evaluation 모드로 변경한다. Evaluation 모드에서는 dropout, batch normalization와 같이 학습할 때만 적용해야하는 레이어 기능을 비활성화 시킨다. gen.zero_grad()는 역전파 과정을 실행하기 전 각 parameter의 gradient를 0으로 초기화한다. 모델의 가중치를 갱신하기 전에 적용한다.Channel 수가 1인 28 * 28 사이즈의 이미지 데이터를 1차원 데이터로 flatten하여 모델에 대입해주기 위해 reshape(batch_size, -1)를 적용해준다.Generator는 원래 목적 함수 $\\log (1-D(G(z)))$를 최소화하여야 한다. 그러나 원 논문의 3. Adversarial nets 3번째 문단을 보면, $\\log D(G(z))$를 최대화하도록 학습하는 편이 더 좋은 결과를 낳는다고 설명한다. Rather than training $G$ to minimize $\\log (1-D(G(z)))$ we can train $G$ to maximize $\\log D(G(z))$. This objective function results in thesame fixed point of the dynamics of $G$ and $D$ but provides much stronger gradients early in learning.따라서 $BCELoss(D(G(z)), 1) = - \\log D(G(z))$를 최소화하도록 generator를 학습한다.마찬가지로 Discriminator는 목적 함수 $\\log D(x) + \\log (1-D(G(z)))$를 최대화하여야 하므로 $BCELoss(D(x), 1) + BCELoss(D(G(z)), 0)$를 최소화하도록 학습한다.매 epoch마다 generator와 discriminator의 loss를 출력하고 리스트에 저장한다. 또한 real data와 fake data에 대한 discriminator의 예측을 batch 단위로 평균내어 출력하고 리스트에 저장한다. 20 epoch마다 고정된 latent data를 generator에 입력하여 출력되는 새로운 손글씨 데이터를 저장한다.학습을 완료한 후 모델과 학습 그래프를 저장하고, gif 이미지를 생성한다.4. 결과Loss 값이 감소하는 것이 잘 관찰되지 않는다.이론적으로는 prediction 값이 0.5로 수렴해야하나, 이 역시 잘 관찰되지 않는다.20 epoch 별로 나타낸 새로운 손글씨 데이터초반 학습에서 새로운 손글씨 데이터 변화전체적으로 수렴이 잘 진행되지 않는다. Loss 값이나 prediction 값이 전 epoch에 걸쳐 잘 변하지 않고, 학습 종료 후 생성된 데이터와 초반부 학습 때 생성된 데이터가 별반 달라보이지 않는다. 이러한 한계점을 극복하여 성능을 향상시키고자 다양한 GAN 모델이 개발되었다. 그 중에서, 다음 포스팅은 이미지 데이터 학습에 유리한 DCGAN과 CGAN에 대해 정리해볼 것이다." }, { "title": "[GAN] Generative Adversarial Nets 논문 리뷰", "url": "/posts/GAN_review/", "categories": "AI/ML/DL, GAN", "tags": "gan, generative model, generator, discriminator, kld, jsd", "date": "2022-02-14 00:00:00 +0900", "snippet": "GAN은 Generative Adversarial Nets에서 처음 소개된 아키텍쳐이다. GAN은 딥러닝 기반 생성 모델에 매우 큰 영향을 미쳤으며, 오늘날까지 GAN을 기반으로 한 다양한 파생 모델이 등장하고 있다. 이번 포스팅에서는 GAN에 대하여 간단히 정리해 보았다. 본 포스팅은 다음 자료를 참고하였습니다. GAN: Generative Adversarial Networks (꼼꼼한 딥러닝 논문 리뷰와 코드 실습) 1시간만에 GAN(Generative Adversarial Network) 완전 정복하기 생성 모델(Generative Model)생성 모델이란 주어진 학습 데이터의 분포를 학습하여 실존하지 않지만 유사한 데이터를 생성하는 모델을 의미한다. 이때 학습 데이터의 분포는 데이터의 종류에 따라 다르게 생각할 수 있다. 가령 학습 데이터가 이미지라면 이미지의 feature, 즉 사람의 이목구비나 이미지의 명도/채도 등에 해당할 것이고, 오디오라면 목소리의 음색이나 높낮이 등에 해당할 것이다. 학습 데이터의 각 feature를 확률 변수로 생각한다면 하나의 학습 데이터는 parameter space에서 하나의 point에 대응될 것이고, 학습 데이터 그룹은 다변수 확률분포를 따를 것이다. 이러한 분포를 모방한 새로운 데이터를 생성하는 것이 생성 모델의 목적이다.출처: 원 논문적대적 생성 모델(Generative Adversarial Networks, GAN)GAN은 생성자(generator)와 판별자(discriminator) 2개의 네트워크를 적대적으로 맞서 싸우게 하여 학습하는 모델이다. 흔히 GAN 모델은 경찰과 위조 지폐범의 대결로 비유되곤 한다. 위조 지폐범은 최대한 진짜 지폐와 유사하게끔 위조 지폐를 만들어 경찰을 속일 수 있도록 노력한다. 경찰은 지폐와 위조 지폐를 최대한 구분할 수 있도록 노력한다. 이 둘의 경쟁이 지속되면 위조 지폐범은 진짜와 다름 없는 위조 지폐를 만드는 방향으로 수렴할 것이고, 경찰이 두 지폐를 구분할 수 있는 확률도 1/2로 수렴할 것이다. 이때 위조 지폐범은 generator에, 경찰은 discriminator에 대응된다.위조 지폐범은 최대한 진짜와 가까운 지폐를 만들고, 경찰은 최대한 두 지폐를 구분한다.임의의 latent vector $z$에 대해 generator $G$는 latent space에서 학습 데이터의 space로 mapping한다고 하자. 또한 discriminator $D$는 데이터를 입력으로 받아 실제 존재하는 데이터에 가까울수록 1을, 존재하지 않는 데이터에 가깝다고 판단되면 0을 출력한다고 하자. 이때 GAN 아키텍쳐는 다음과 같이 구성된다.출처: GAN: Generative Adversarial Networks (꼼꼼한 딥러닝 논문 리뷰와 코드 실습) 영상 중$G$는 latent vector $z$로부터 fake data를 생성하여 $D$가 이를 real, 1에 가까운 확률을 출력하도록 학습되어야 할 것이다. 따라서 $G$는 $D(G(z))$가 최대화하는 방향으로 학습되어야 한다. 반대로 $D$는 fake data는 0에 가까운 확률을, real data는 1에 가까운 확률을 출력하도록 학습되어야 할 것이다. 따라서 $D$는 $D(G(z))$를 최소화, $G(x)$를 최대화하는 방향으로 학습되어야 한다. 이 두 사실을 log-loss function으로 묶어 정리하면 다음 수식을 얻는다.\\[\\min_{G} \\max_{D} V(G, D) = \\mathbb{E}_{x \\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E}_{x \\sim p_{z}(z)}[\\log(1-D(G(z)))]\\]이론적 뒷받침크게 2가지를 보일 것이다. 첫 번째는 generator $G$가 만드는 확률 분포가 데이터의 확률 분포에 수렴한다는 점이고, 두 번째는 $D(G(z))=1/2$, 다시 말해 $G$가 만드는 fake data와 real data를 $D$가 구분할 수 없는 방향으로 수렴한다는 점이다.Proposition 1. 주어진 $G$에 대해, 최적 discriminator $D$는 $D_{G}^{*}(x) = \\frac {p_{data}(x)} {p_{data}(x) + p_g (x)}$이다.$Proof.$\\[\\begin{align}V(G, D) &amp;amp;= \\mathbb{E}_{x \\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log(1-D(G(z)))] \\\\&amp;amp;= \\int_{x} p_{data}(x) \\log(D(x)) dx + \\int_{z} p_{z}(z) \\log(1-D(G(z))) dz \\\\&amp;amp;=\\int_{x} p_{data}(x) \\log(D(x)) + p_{g}(x) \\log(1-D(x)) dx\\end{align}\\]연속확률분포의 기댓값 공식에 의해 첫번째 줄에서 두번째 줄로 넘어갈 수 있고, $G$는 latent space를 data space로 mapping하므로 두번째 줄에 치환 적분을 적용하여 세번째 줄로 넘어간다.이때 함수 $y = a \\log y + b \\log (1-y)$는 $y = \\frac{a}{a+b}$에서 유일한 최댓값을 가진다. 따라서 $D$는 유일한 최적값 $D_{G}^{*}(x) = \\frac {p_{data}(x)} {p_{data}(x) + p_g (x)}$이 존재한다.Theorem 1. $\\max_D V(G, D)$는 오직 $p_g=p_{data}$일 때 최솟값을 갖고, 그 값은 $–\\log 4$이다.$proof.$\\[\\begin{align}\\max_{D} V(G, D) &amp;amp;= \\mathbb{E}_{x \\sim p_{data}(x)} [\\log D^*(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log(1-D^*(G(z)))]\\\\&amp;amp;= \\mathbb{E}_{x \\sim p_{data}(x)} \\left[ \\log \\frac {p_{data}(x)} {p_{data}(x) + p_g (x)} \\right] + \\mathbb{E}_{x \\sim p_{g}(x)}\\left[\\log\\frac {p_{g}(x)} {p_{data}(x) + p_g (x)}\\right]\\\\&amp;amp;= \\mathbb{E}_{x \\sim p_{data}(x)} \\left[ \\log \\frac {2 p_{data}(x)} {p_{data}(x) + p_g (x)} \\right] + \\mathbb{E}_{x \\sim p_{g}(x)}\\left[\\log\\frac {2 p_{g}(x)} {p_{data}(x) + p_g (x)}\\right] - \\log 4 \\\\&amp;amp;= KL\\left(p_{data} || \\frac{p_{data}(x) + p_g (x)}{2}\\right) + KL\\left(p_{g} || \\frac{p_{data}(x) + p_g (x)}{2}\\right) - \\log 4 \\\\&amp;amp;= 2 JSD(p_{data} || p_{g}) - \\log 4\\end{align}\\]이때 $KL(p || q)=\\int p \\log \\frac{p}{q}$는 쿨백-라이블러 발산(Kullback–Leibler divergence, KLD)으로, 한쪽에 근사한 확률 분포가 원본과 얼마나 비슷한지 정보 엔트로피 관점으로 측정하는 함수이다. 두 대등한 확률 분포가 얼마나 닮았는지 측정하기 위해 KL-divergence를 symmetric하게 계산한 $JSD(p || q)=\\frac{1}{2} \\left(KL(p || \\frac{p+q}{2}) + KL(q || \\frac{p+q}{2}) \\right) $ 로 정의하는데, 이를 Jensen-Shannon divergence라고 한다.$JSD(p || q)$는 $p, q$의 확률분포가 동일할 때 최솟값 0을 가지므로 $\\max_{D} V(G, D)$는 오직 $p_g=p_{data}$일 때 최솟값을 갖고, 그 값은 $–\\log 4$임이 증명된다. $p_g = p_{data}$일 때 $D_{G}^* = \\frac{1}{2}$이므로 $D(G(z)) \\rightarrow \\frac{1}{2}$로 수렴한다는 사실도 확인할 수 있다." }, { "title": "[Multiple View Geometry in Computer Vision] 0. Foreword, Preface", "url": "/posts/Multiple_View_Geometry_0/", "categories": "전공서적 리딩, Multiple View Geometry in Computer Vision", "tags": "multiple view geometry in computer vision, computer vision, cv, foreword, preface", "date": "2022-02-07 00:00:00 +0900", "snippet": "Computer vision, SLAM 분야를 공부해보고자 교수님께 여쭤봤더니 Multiple View Geometry in Computer Vision 을 무조건 읽어보라고 추천해 주셨다. 이번 포스팅부터 전공 책을 읽고 나름대로 요약한 내용을 정리해볼까 한다.Foreword 컴퓨터의 시각 처리 기술(Making a computer see)은 AI 학계에서 여름 학생들의 프로젝트 정도로 가볍게 여겨졌지만, 아직까지 해결되지 못한 난제임. CV 분야는 수학, 컴퓨터 과학과 강한 관련이 있고, 물리, 심리학, 신경학과 약한 연관이 있음. 생물학적으로 시각 처리 작업은 많이 알려져 있지 않아 CV 분야에서 모방하기 어렵지만, 이러한 요소를 무시한 시도는 성공적이지 못함. CV 분야의 성취는 실무적 측면과 이론적 측면으로 나눌 수 있음. 실무적 측면의 사례로, 차량을 일반 도로나 거친 노면에서 안내하는 기술이 있는데, 이는 매우 정교한 실시간 3D dynamic scene 분석 기술을 필요로 함. 이론적 측면에서 geometric Computer Vision 라는 분야가 성취됨. e.g. 시점에 따른 물체의 appearance를 물체의 모양과 카메라 parameters의 함수로 묘사. 이러한 종류의 작업이 컴퓨터의 시각 처리 기술을 위한 옳은 방향이 맞는지에 대한 질문은 독자에게 남김.Preface CV 중 geometry of multiple views 분야는 매우 빠르게 발전. 2장의 이미지가 주어지고, 다른 정보가 없을 때, 이미지 사이의 matching을 계산하고 이미지를 생선한 카메라의 3D 상 위치를 계산. 3장 이미지가 주어지고, 다른 정보가 없을 때, 유사하게 이미지의 점, 선들의 매칭을 계산하고, 카메라의 위치 계산. Stereo rig의 epipolar geometry와 trinocular rig의 trifoal geometry를 보정 없이 계산. 자연 장면에서의 연속적인 이미지로부터 카메라 내부 보정 계산. 이런 알고리즘의 독특한 점은 uncalibrated (카메라 parameter 등을 알 필요가 없음). 알고리즘은 view로부터 상이 맺힌 점, 선들에 의한 contraints와 각 이미지에 대응되는 3차원 상 카메라의 위치에 기반. e.g. Stereo rig의 epipolar geometry를 계산할 때는 카메라 보정을 제외한 7개의 parameters만 필요. 과거의 알고리즘은 각각의 카메라가 보정이 필요했고, 이러한 보정은 11개의 parameters가 요구됨. 적절한 geometry representation을 사용한 uncalibrated 접근은 알고리즘 각 단계에 필요한 parameters를 명시. 더 간단한 알고리즘. 3차원 상 점과 같은 entities의 ambiguity를 복구함. 이론적 측면에서, 종종 보정이 불가능한 상황(카메라가 이동, 카메라 내부 parameters가 변화 등)에도 적용 가능. 이라는 이점이 있음. 이러한 성취를 거두기 위해 Over-determined system에서 error 최소화 기법. RANSAC과 같은 robust한 estimation 사용. 와 같은 수학적 접근을 사용. Reconstruction 문제 역시 우리가 풀었다고 주장할만한 수준까지 발전. 이미지 점들의 대응으로부터 multifocal 텐서 추정. 이러한 텐서로부터 camera matrices 추출, 2, 3, 4 views로부터 subsequent projective 재구성. 유의미한 성취가 있지만, 아직 더 연구할 문제가 존재. 일반적인 reconstruction 문제 해결을 위한 bundle adjustment 적용. 카메라 행렬에 대한 최소한의 가정 하에 metric 재구성. multifocal tensor 관계를 이용하여 이미지 sequence의 대응 관계 자동 감지, 이상값, false matches 제거. 목차 설명 Part 0 Background: 2-space, 3-space에서 projective geometry, 이러한 geometry의 표현, 조작, 추정 방법, geometry와 CV의 다양한 목표 사이의 관계 설명. Part 1 Single view geometry: 3-space에서 perspective projection을 모델링하는 다양한 카메라를 정의, 보정된 물체에 전통적인 기술을 사용한 추정, 소실점과 소실선에서의 카메라 보정에 대해 학습. Part 2 Two view geometry: 두 카메라의 epipolar geometry, 이미지 점 대응으로부터 projective reconstruction, projective ambiguity 해결 방법, 최적 triangulation 등에 대해 학습. Part 3 Three view geometry: 세 카메라의 trifocal geometry, two views에서 three views로 점, 선 대응, 점과 선의 대응에서 geometry 계산, 카메라 행렬 등 학습. Part 4 N-views: three view에서 four views로 확장, N-views에 적용 가능한 Tomasi, Kanade factorization 알고리즘 학습, 앞선 장을 확장한 공통적인 내용을 다룸. e.g. multi-linear view contraints, auto-calibration, ambiguous solutions " } ]
